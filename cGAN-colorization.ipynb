{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcXMbUxdJyJtkVx+pBCSy9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 1 Data collection"],"metadata":{"id":"e54QtGM-4FGH"}},{"cell_type":"markdown","source":["Two datasets are used: a small version of COCO dataset with 21,837 images and one with 17,178 images of animals (12 categories)"],"metadata":{"id":"Sk3EcePY4PV1"}},{"cell_type":"markdown","source":["##1.1 Animals dataset"],"metadata":{"id":"1B0UnTf94ndc"}},{"cell_type":"markdown","source":["We download this dataset from kaggle (1.4 GB)"],"metadata":{"id":"QbIXQR5B5Qty"}},{"cell_type":"code","source":["!pip install -q kaggle\n","from google.colab import files"],"metadata":{"id":"5LyAJsIy4H9R","executionInfo":{"status":"ok","timestamp":1672481690056,"user_tz":-60,"elapsed":4363,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"],"metadata":{"id":"gW1I4x0j5dLh"}},{"cell_type":"code","source":["files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"HcwM9NAY5cJQ","executionInfo":{"status":"ok","timestamp":1672481700650,"user_tz":-60,"elapsed":10611,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"cf69c3d9-e43a-468b-ef12-82142f1432f1"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-72174a98-7b97-42ed-ad19-6178b230342c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-72174a98-7b97-42ed-ad19-6178b230342c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"riccardodemonteita\",\"key\":\"189ea3d8a51099270c4759aab3b1cecd\"}'}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! kaggle datasets list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgTBTr6f5yv8","executionInfo":{"status":"ok","timestamp":1672481702928,"user_tz":-60,"elapsed":703,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"a758fdd1-2e86-4fb3-d233-af8ee60bccad"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","ref                                                             title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","--------------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","meirnizri/covid19-dataset                                       COVID-19 Dataset                                 5MB  2022-11-13 15:47:17          14442        413  1.0              \n","thedevastator/analyzing-credit-card-spending-habits-in-india    Credit Card Spending Habits in India           319KB  2022-12-14 07:30:37           1279         49  1.0              \n","michals22/coffee-dataset                                        Coffee dataset                                  24KB  2022-12-15 20:02:12           3433         77  1.0              \n","thedevastator/unlock-profits-with-e-commerce-sales-data         E-Commerce Sales Dataset                         6MB  2022-12-03 09:27:17           2673         65  1.0              \n","thedevastator/jobs-dataset-from-glassdoor                       Salary Prediction                                3MB  2022-11-16 13:52:31           8170        178  1.0              \n","die9origephit/fifa-world-cup-2022-complete-dataset              Fifa World Cup 2022: Complete Dataset            7KB  2022-12-18 22:51:11           2893        107  1.0              \n","mattop/highest-grossing-mobile-games                            Highest Grossing Mobile Games                    3KB  2022-12-19 15:20:22            675         30  1.0              \n","thedevastator/uncover-global-trends-in-mental-health-disorder   Global Trends in Mental Health Disorder          1MB  2022-12-14 05:30:38            970         31  1.0              \n","rajkumarpandey02/fifa-world-cup-attendance-19302022             FIFA World Cup Attendance 1930-2022              5KB  2022-12-19 10:04:26            930         25  1.0              \n","devrimtuner/number-of-road-motor-vehicles-turkey                Number of road motor vehicles ðŸ‡¹ðŸ‡·                 3KB  2022-12-25 13:04:17            279         23  0.9411765        \n","thedevastator/revealing-insights-from-youtube-video-and-channe  YouTube Videos and Channels Metadata            82MB  2022-12-14 02:48:24            675         35  1.0              \n","thedevastator/uncovering-insights-to-college-majors-and-their   College Majors and their Graduates              39KB  2022-12-06 16:06:52           1411         41  1.0              \n","mvieira101/global-cost-of-living                                Global Cost of Living                            1MB  2022-12-03 16:37:53           3885         79  0.9705882        \n","anashamoutni/students-employability-dataset                     Students' Employability Dataset - Philippines   97KB  2022-12-18 15:51:39            818         30  0.88235295       \n","swaptr/fifa-world-cup-2022-statistics                           FIFA World Cup 2022 Team Data                   15KB  2022-12-19 00:29:15           2861         64  0.9705882        \n","thedevastator/the-ultimate-netflix-tv-shows-and-movies-dataset  Netflix TV Shows and Movies (2022 Updated)       2MB  2022-11-27 20:41:41           2835         53  1.0              \n","whenamancodes/predict-diabities                                 Predict Diabetes                                 9KB  2022-11-09 12:18:49           8499        134  1.0              \n","kulturehire/understanding-career-aspirations-of-genz            Understanding Career Aspirations of GenZ         8KB  2022-12-21 13:44:32            381         25  0.9117647        \n","thedevastator/uncovering-wage-disparities-in-pennsylvania-s-hi  Higher Education Wages                         223KB  2022-12-04 15:42:36           1419         46  1.0              \n","laibaanwer/superstore-sales-dataset                             SuperStore Sales Dataset                         2MB  2022-12-07 08:53:32           1765         40  1.0              \n"]}]},{"cell_type":"code","source":["!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4X-z45AP54t6","executionInfo":{"status":"ok","timestamp":1672481718825,"user_tz":-60,"elapsed":14224,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"978e2fe0-d9df-49e8-d6bf-d1933c5cede5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading animal-image-classification-dataset.zip to /content\n"," 99% 1.45G/1.47G [00:13<00:00, 98.4MB/s]\n","100% 1.47G/1.47G [00:13<00:00, 120MB/s] \n"]}]},{"cell_type":"markdown","source":["The data have been downloaded. To unzip them"],"metadata":{"id":"Ep7psYmO6Dmv"}},{"cell_type":"code","source":["!mkdir /content/animal_data\n","!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"],"metadata":{"id":"ldwePf3c59cJ","executionInfo":{"status":"ok","timestamp":1672481732143,"user_tz":-60,"elapsed":13327,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Now we have to split it on two: training dataset (used to train the classifier) and validation/test dataset. Before you have to upload \"val_animals.txt\" that allows to split the dataset (two list with the paths will be obtained) "],"metadata":{"id":"pFLRUii77KN4"}},{"cell_type":"code","source":["files.upload();"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"FwMAXUWN6Hsw","executionInfo":{"status":"ok","timestamp":1672489938808,"user_tz":-60,"elapsed":7506,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"f77d5089-b18c-4b00-83fc-fa78b6e6caad"},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-90474886-8ff2-46e5-8064-c0ec6c6eb71d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-90474886-8ff2-46e5-8064-c0ec6c6eb71d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving val_animals.txt to val_animals.txt\n"]}]},{"cell_type":"code","source":["import os\n","#set to None to use all the images (14K)\n","max_img_per_class = 100\n","\n","path = \"/content/animal_data\"\n","animal_path = path + \"/Animal Image Dataset\"\n","\n","animals = [\"butterfly\", \"cats\", \"cow\", \"dogs\", \"elephant\", \"hen\", \"horse\", \"monkey\", \"panda\", \"sheep\", \"spider\", \"squirrel\"]\n","\n","train_paths = []\n","#labels\n","train_labels = []\n","val_paths = []\n","val_labels = []\n","\n","#collect paths validation/test images\n","with open(\"/content/val_animals.txt\") as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","#collect the corresponding labels\n","for path in val_paths:\n","\n","  for i, animal in enumerate(animals):\n","\n","    if animal in path:\n","\n","      val_labels.append(i)\n","      break\n","\n","\n","#build training dataset:max_img_per_class images for each class (excluding the ones in the validation dataset)\n","#if None, all the images\n","\n","for i,animal in enumerate(animals):\n","  counter = 0\n","  folder = os.listdir(animal_path+\"/\"+animal) \n","\n","  for image in folder:\n","    \n","\n","    if max_img_per_class == None:\n","    \n","      if animal_path+\"/\"+animal+\"/\"+image not in val_paths:\n","\n","        train_paths.append(animal_path+\"/\"+animal+\"/\"+image)\n","        train_labels.append(i)\n","\n","    else:\n","\n","      if counter == max_img_per_class:\n","\n","        break\n","\n","      if animal_path+\"/\"+animal+\"/\"+image not in val_paths:\n","\n","        train_paths.append(animal_path+\"/\"+animal+\"/\"+image)\n","        train_labels.append(i)\n","        counter +=1\n","\n","\n","print(f\"# training images: {len(train_paths)}\\n# val/test images: {len(val_paths)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxF-mGEU7ocS","executionInfo":{"status":"ok","timestamp":1672481742673,"user_tz":-60,"elapsed":209,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"f36bd0ca-812d-4868-98c6-189287b74187"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["# training images: 1200\n","# val/test images: 2400\n"]}]},{"cell_type":"markdown","source":["## 1.2 COCO dataset"],"metadata":{"id":"5sILCvtJ8rbC"}},{"cell_type":"markdown","source":["To download it we use fastai"],"metadata":{"id":"grBxE-GZ8v1g"}},{"cell_type":"code","source":["!pip install fastai==2.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Of1FTMIW8VXG","executionInfo":{"status":"ok","timestamp":1672481875338,"user_tz":-60,"elapsed":123175,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"6b779097-48bf-4c51-a586-cb21650088f3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fastai==2.4\n","  Downloading fastai-2.4-py3-none-any.whl (187 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (3.2.2)\n","Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (7.1.2)\n","Collecting torch<1.10,>=1.7.0\n","  Downloading torch-1.9.1-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 831.4 MB 9.5 kB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (6.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.7.3)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (0.14.0+cu116)\n","Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (21.1.3)\n","Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.0.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.3.5)\n","Collecting fastcore<1.4,>=1.3.8\n","  Downloading fastcore-1.3.29-py3-none-any.whl (55 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (2.23.0)\n","Requirement already satisfied: spacy<4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (3.4.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (21.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.10.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (8.1.5)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.3.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (57.4.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.0.4)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.7.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.4.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (4.64.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.0.9)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.10.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.0.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.0.10)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (6.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->fastai==2.4) (3.0.9)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4->fastai==2.4) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (1.24.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.4) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.4) (0.0.3)\n","Collecting torchvision>=0.8.2\n","  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2 MB 1.1 MB/s \n","\u001b[?25h  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.3 MB 58.7 MB/s \n","\u001b[?25h  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.1 MB 1.1 MB/s \n","\u001b[?25h  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.1 MB 1.1 MB/s \n","\u001b[?25h  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0 MB 1.1 MB/s \n","\u001b[?25h  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.2 MB 1.3 MB/s \n","\u001b[?25h  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.3 MB 1.3 MB/s \n","\u001b[?25h  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.3 MB 1.3 MB/s \n","\u001b[?25h  Downloading torchvision-0.10.1-cp38-cp38-manylinux1_x86_64.whl (22.1 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.1 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai==2.4) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4->fastai==2.4) (2.0.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==2.4) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai==2.4) (2022.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.4) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.4) (3.1.0)\n","Installing collected packages: torch, torchvision, fastcore, fastai\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.0+cu116\n","    Uninstalling torchvision-0.14.0+cu116:\n","      Successfully uninstalled torchvision-0.14.0+cu116\n","  Attempting uninstall: fastcore\n","    Found existing installation: fastcore 1.5.27\n","    Uninstalling fastcore-1.5.27:\n","      Successfully uninstalled fastcore-1.5.27\n","  Attempting uninstall: fastai\n","    Found existing installation: fastai 2.7.10\n","    Uninstalling fastai-2.7.10:\n","      Successfully uninstalled fastai-2.7.10\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.9.1 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n","Successfully installed fastai-2.4 fastcore-1.3.29 torch-1.9.1 torchvision-0.10.1\n"]}]},{"cell_type":"code","source":["from fastai.data.external import untar_data, URLs\n","import os\n","import glob\n","import numpy as np"],"metadata":{"id":"PGmlvToL83YY","executionInfo":{"status":"ok","timestamp":1672481877384,"user_tz":-60,"elapsed":2067,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","\n","paths = glob.glob(coco_path+\"/*.jpg\")\n","paths =np.array(paths)\n","num_images_coco = len(paths)\n","print(f\"# coco images: {num_images_coco}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"0oQGGmVW83uq","executionInfo":{"status":"ok","timestamp":1672481958992,"user_tz":-60,"elapsed":81612,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"cdfbadf4-6191-4d39-e81b-43ed09f757c5"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["# coco images: 21837\n"]}]},{"cell_type":"markdown","source":["# 2 Datasets and Dataloaders"],"metadata":{"id":"5LOVOSm0b9QU"}},{"cell_type":"code","source":["from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"_w-5Nc7mgY38","executionInfo":{"status":"ok","timestamp":1672481959551,"user_tz":-60,"elapsed":562,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## 2.1 Training Dataset"],"metadata":{"id":"Ya73EyyAkVV3"}},{"cell_type":"markdown","source":["We select a subset of COCO"],"metadata":{"id":"qFZgRf1OnApe"}},{"cell_type":"code","source":["idxs = np.random.permutation(num_images_coco)\n","\n","n_train_samples_coco = 14800\n","\n","coco_train_idxs = idxs[:n_train_samples_coco]\n","\n","coco_train_paths = paths[coco_train_idxs]"],"metadata":{"id":"5SbGgJpJkST1","executionInfo":{"status":"ok","timestamp":1672481959552,"user_tz":-60,"elapsed":6,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["animals_paths = np.array(train_paths)"],"metadata":{"id":"lJb0oKGin7Lm","executionInfo":{"status":"ok","timestamp":1672482142764,"user_tz":-60,"elapsed":212,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["union of the two datasets"],"metadata":{"id":"CnWA4Xv3qB7U"}},{"cell_type":"code","source":["training_paths = np.concatenate([coco_train_paths, animals_paths])"],"metadata":{"id":"WvFAixJEoVW6","executionInfo":{"status":"ok","timestamp":1672482143458,"user_tz":-60,"elapsed":1,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(f\"# training images: {training_paths.shape[0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5H4pxuWpIJU","executionInfo":{"status":"ok","timestamp":1672482144072,"user_tz":-60,"elapsed":257,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"088ac663-2a3a-4836-e48b-891e6851abf6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["# training images: 16000\n"]}]},{"cell_type":"code","source":["idxs = np.random.permutation(training_paths.shape[0])\n","\n","training_paths = training_paths[idxs]"],"metadata":{"id":"AErKk23dpVL3","executionInfo":{"status":"ok","timestamp":1672482144349,"user_tz":-60,"elapsed":2,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["SIZE = 256\n","\n","train_transform = transforms.Compose([\n","                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n","                transforms.RandomHorizontalFlip(),\n","            ])"],"metadata":{"id":"YyNuqjo6peDz","executionInfo":{"status":"ok","timestamp":1672482144858,"user_tz":-60,"elapsed":7,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class GrayToColorDataset(Dataset):\n","\n","  def __init__(self, paths, transform = None):\n","    \n","    self.paths = paths\n","    self.transform = transform\n","\n","  def __len__(self):\n","\n","    return len(self.paths)\n","\n","  def __getitem__(self, idx):\n","\n","    img_rgb = Image.open(self.paths[idx]).convert(\"RGB\")\n","    img_rgb = self.transform(img_rgb)\n","    img_rgb = np.array(img_rgb)\n","\n","    #RGB -> Lab\n","    img_lab = rgb2lab(img_rgb).astype(\"float32\")\n","    img_lab = transforms.ToTensor()(img_lab)\n","\n","    #to have values in range [-1,1]\n","    L = img_lab[0,:]/50. - 1.\n","    ab = img_lab[[1,2],:] / 110.\n","\n","    return (L.unsqueeze(0),ab)\n"],"metadata":{"id":"UtCJVViBrLMK","executionInfo":{"status":"ok","timestamp":1672482146967,"user_tz":-60,"elapsed":3,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["train_dataset = GrayToColorDataset(training_paths, train_transform)"],"metadata":{"id":"rwCDspxZrYlp","executionInfo":{"status":"ok","timestamp":1672482148617,"user_tz":-60,"elapsed":204,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n","PIN_MEMORY = True\n","N_WORKERS = 2\n","BATCH_SIZE = 32\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n","                            pin_memory=PIN_MEMORY, shuffle = True)"],"metadata":{"id":"222dWNwbrpeJ","executionInfo":{"status":"ok","timestamp":1672482236410,"user_tz":-60,"elapsed":200,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Test Dataset"],"metadata":{"id":"j96HijJqr2eE"}},{"cell_type":"code","source":[],"metadata":{"id":"ixCb6qElr-2g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 cGAN models"],"metadata":{"id":"WNsZnfNfsiBg"}},{"cell_type":"markdown","source":["## 3.1 Generator: U-Net"],"metadata":{"id":"NZkM2-UhsvBp"}},{"cell_type":"code","source":["class UNetDown(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4, normalization_type = None, dropout = 0.0, activation = None):\n","\n","    super(UNetDown, self).__init__()\n","\n","    #if batchnorm/instancenorm used, bias not used\n","\n","    use_bias = normalization_type == None\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","        \n","    if activation == None:\n","      layers.append(nn.LeakyReLU(negative_slope = 0.2))\n","\n","    if activation == \"ReLU\":\n","\n","      layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x):\n","\n","    return self.model(x)\n"],"metadata":{"id":"4Vnxybgnsr2Y","executionInfo":{"status":"ok","timestamp":1672482288727,"user_tz":-60,"elapsed":198,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class UNetUp(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4,  normalization_type = None, dropout = 0.0):\n","\n","    super(UNetUp, self).__init__()\n","\n","    use_bias = normalization_type == None\n","\n","    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","\n","    layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x, skip = None):\n","      x = self.model(x)\n","      if skip is not None:\n","\n","        x = torch.cat((skip, x), 1)\n","\n","      return x"],"metadata":{"id":"QqTa1lzVtnwT","executionInfo":{"status":"ok","timestamp":1672482300823,"user_tz":-60,"elapsed":2,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class GeneratorUNet(nn.Module):\n","\n","  def __init__(self, in_channels = 1, out_channels = 2, num_down = 8, ngf = 64, normalization_type = None):\n","\n","    super(GeneratorUNet, self).__init__()\n","\n","    self.downs = nn.ModuleList()\n","    self.ups = nn.ModuleList()\n","    \n","\n","    features =[ngf]\n","\n","    for i in range(3):\n","\n","      features.append(features[i]*2)\n","\n","    features.append(features[-1])\n","    #64, 128, 256, 512, 512\n","\n","    if num_down > 5:\n","\n","      features += [ngf * 8 for i in range(num_down - 5)]\n","    #for num_down = 8: 64, 128, 256, 512, 512, 512, 512, 512 (1x1 for input size 256x256)\n","\n","\n","    #ENCODER (CONTRACTING PATH)\n","\n","    #outermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, ngf, 4))\n","\n","    in_channels = ngf #new in_channels for the next down block\n","    \n","    for i,n_features in enumerate(features[1:len(features)-1]):\n","      #no dropout\n","      self.downs.append(UNetDown(in_channels, n_features, 4, normalization_type, 0.0))\n","      in_channels = n_features\n","\n","    \n","    #innermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, features[-1], 4, activation = \"ReLU\"))\n","    \n","\n","    #DECODER (EXPANSIVE PATH)\n","    i_channels = in_channels\n","    for i, n_features in enumerate((features[-2::-1])):\n","      \n","      #print(n_features)\n","      #if i == 0, innermost(bottleneck), namely a block such that after down we go up. no dropout\n","      i_channels = in_channels if i == 0  else i_channels * 2\n","\n","      #no dropout for the first up and the last 4 ups \n","      dropout = 0.0 if (i == 0 or i  > 3) else 0.5\n","\n","      self.ups.append(UNetUp(i_channels, n_features, 4, normalization_type, dropout))\n","      i_channels = n_features\n","    \n","    \n","    self.final = nn.Sequential(\n","        nn.ConvTranspose2d(ngf*2,out_channels, kernel_size=4, stride=2, padding=1),\n","        nn.Tanh()\n","    )\n","\n","\n","\n","  def forward(self, x):\n","\n","    skip_connections = list()\n","\n","    #encoder\n","    for down in self.downs:\n","\n","      x = down(x)\n","      skip_connections.append(x)\n","\n","    #decoder with skip connections\n","    for i, up in enumerate(self.ups):\n","      \n","      x = up(x, skip_connections[-i-2])\n","\n","    return self.final(x)"],"metadata":{"id":"4UQKVsqrtqtM","executionInfo":{"status":"ok","timestamp":1672482524338,"user_tz":-60,"elapsed":2,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#da cancellare\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","G = GeneratorUNet(1,2,8,64, \"batchnorm\").to(device)\n","\n","G.eval()\n","\n","print(G(train_dataset[0][0].unsqueeze(0).to(device)).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kynfhjagtxrD","executionInfo":{"status":"ok","timestamp":1672482500911,"user_tz":-60,"elapsed":1180,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"28b40d3e-4e40-406b-aabb-4792181c8109"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[64, 128, 256, 512, 512]\n","torch.Size([1, 2, 256, 256])\n"]}]},{"cell_type":"markdown","source":["## 3.2 Discrimintor: PatchGAN"],"metadata":{"id":"EakwAjpstItC"}},{"cell_type":"markdown","source":["The descriminator is a PatchGAN for $N \\times N$ patches where $N=70$: given an input $256 \\times 256$ the output is $30 \\times 30$"],"metadata":{"id":"SfSxSbH5-fle"}},{"cell_type":"code","source":["class PatchDiscriminator(nn.Module):\n","\n","  def __init__(self, in_channels = 3, ndf = 64, n_down = 5):\n","\n","    super(PatchDiscriminator, self).__init__()\n","\n","    features = [ndf * 2**i for i in range(n_down-1)]\n","\n","    layers = []\n","\n","\n","\n","    for i in range(len(features)):\n","      use_bias = True if i < 1  else False\n","      stride = 2 if i < (len(features)-1) else 1\n","      layers.append(nn.Conv2d(in_channels, features[i], 4, stride, 1, bias = use_bias))\n","\n","      if not use_bias:\n","\n","        layers.append(nn.BatchNorm2d(features[i]))\n","\n","      layers.append(nn.LeakyReLU(0.2))\n","\n","      in_channels = features[i]\n","    \n","    layers.append(nn.Conv2d(in_channels, 1, 4, 1, 1))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","\n","    return self.model(x)"],"metadata":{"id":"gIK5GgDytMoi","executionInfo":{"status":"ok","timestamp":1672483489689,"user_tz":-60,"elapsed":196,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["#da cancellare\n","D = PatchDiscriminator()\n","\n","print(D)\n","\n","prova = torch.randn(1,3, 256,256)\n","print(D(prova).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zj-st_ufyLuC","executionInfo":{"status":"ok","timestamp":1672487412675,"user_tz":-60,"elapsed":616,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"8a942ccb-ace3-432c-95bd-68d1f12e4e81"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["PatchDiscriminator(\n","  (model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2)\n","    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n","  )\n",")\n","torch.Size([1, 1, 30, 30])\n"]}]},{"cell_type":"markdown","source":["# 4 GAN LOSS"],"metadata":{"id":"YQ8ki3pfyrz5"}},{"cell_type":"markdown","source":["The following class allows to implement the GAN loss: for the discriminator \n","\\begin{equation}\n","\\mathbb{E}_{x,y}[\\log D(x,y)]+\\mathbb{E}_{x,z}[\\log(1-D(x,G(z, x)))]\n","\\end{equation}\n","\n","For the generator instead\n","\n","\\begin{equation}\n","\\mathbb{E}_{x,z}[\\log D(x,G(z,x))]\n","\\end{equation}"],"metadata":{"id":"P6gdCkc0Bamk"}},{"cell_type":"code","source":["class GANLoss():\n","\n","  def __init__(self, device):\n","\n","    self.criteria = nn.BCEWithLogitsLoss()\n","    self.real = 1.\n","    self.fake = 0.\n","    self.device = device\n","\n","  def __call__(self, input, label_type):\n","    \n","    label = torch.tensor(self.real if label_type else self.fake)\n","    \n","    labels = label.expand_as(input).to(self.device)\n","    \n","    return self.criteria(input, labels)"],"metadata":{"id":"ya2IZzYjyRg0","executionInfo":{"status":"ok","timestamp":1672486946332,"user_tz":-60,"elapsed":210,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["#da cancellare\n","loss_fn = GANLoss(device)\n","sig = nn.Sigmoid()\n","prova = torch.tensor([ [ [[5, 1.0], [2, 1]]  ]])\n","\n","print(loss_fn(prova.to(device), True))\n","print(-torch.mean(torch.log(sig(prova))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5TBOQwcy_w8","executionInfo":{"status":"ok","timestamp":1672489224782,"user_tz":-60,"elapsed":411,"user":{"displayName":"Riccardo De Monte","userId":"15914592007319934058"}},"outputId":"3622d3f3-0b7a-45ba-980a-46435b52f03d"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.)\n","tensor([[[[1., 1.],\n","          [1., 1.]]]], device='cuda:0')\n","tensor(0.1900, device='cuda:0')\n","tensor(0.1900)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Iijz4ikQ-YSV"},"execution_count":null,"outputs":[]}]}