{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k3pQJI9r43Yg"},"outputs":[],"source":["!pip install -q kaggle\n","from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"gW1I4x0j5dLh"},"source":["You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEdcBCvh47ho"},"outputs":[],"source":["files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-QO5dHK49gL"},"outputs":[],"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! kaggle datasets list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJndWr3L5kGj"},"outputs":[],"source":["!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z0UqQMd5B1q"},"outputs":[],"source":["!mkdir /content/animal_data\n","!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EScYGJQbHRu0"},"outputs":[],"source":["from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","import cv2\n"]},{"cell_type":"markdown","metadata":{"id":"n3mYxdQ3KDeP"},"source":["## Building training set and validation set\n"]},{"cell_type":"markdown","source":["### Selecting the images from the entire dataset"],"metadata":{"id":"QA1gBTgBzgmB"}},{"cell_type":"markdown","source":["Upload the file \"test_animals.txt\""],"metadata":{"id":"7AivNXhxjRx_"}},{"cell_type":"code","source":["files.upload();"],"metadata":{"id":"kTA-YrsCtEgW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# path_noadd = path of the txt file with the paths of the images not to add\n","# return the dataframe\n","\n","def build_dataset(path_noadd):\n","\n","  #starting path for the kaggle dataset\n","  start_path = '/content/animal_data/Animal Image Dataset/'\n","\n","  with open(path_noadd) as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","  import os\n","\n","  images = []\n","  labels = []\n","\n","  for folders, subfolders, files in os.walk(start_path,topdown=True):\n","    label = folders.split('/')[4]\n","    for file in files:\n","\n","      path_file = start_path + label + '/' + file\n","\n","      if path_file not in val_paths:  \n","        images.append(path_file)\n","        labels.append(label)\n","      \n","  data = {'Images':images, 'Labels':labels} \n","  data = pd.DataFrame(data) \n","\n","  lb = LabelEncoder()\n","  data['encoded_labels'] = lb.fit_transform(data['Labels'])\n","\n","  return data"],"metadata":{"id":"xHynx5gZwy_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = build_dataset(path_noadd = \"test_animals.txt\")\n","print(len(data))"],"metadata":{"id":"obhNP7NJ2rQM","executionInfo":{"status":"ok","timestamp":1675705074789,"user_tz":-60,"elapsed":351,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"720d5056-b69c-4666-c77d-878913ce51e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["14778\n"]}]},{"cell_type":"code","source":["batch_size = 128\n","train_dim = 12800\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(data)\n","indices = list(range(dataset_size))\n","#split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[:train_dim], indices[train_dim:]\n","\n","print(f\"train dataset length = {len(train_indices)}\")\n","print(f\"validation dataset length = {len(val_indices)}\")"],"metadata":{"id":"gnW6ht9T0BPi","executionInfo":{"status":"ok","timestamp":1675705075018,"user_tz":-60,"elapsed":2,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5555fe0-3478-4f71-9641-ae5c294f2b97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train dataset length = 12800\n","validation dataset length = 1978\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWcPl5ReUBoB"},"outputs":[],"source":["# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SJ6MWSrUhgE"},"outputs":[],"source":["transform = transforms.Compose([\n","                  transforms.Resize((224,224)),\n","                  transforms.ToTensor(),\n","                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","                  ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x42I_5S8UeaG"},"outputs":[],"source":["class Animals_Dataset(Dataset):\n","    def __init__(self, img_data,transform=None):\n","        self.transform = transform\n","        self.img_data = img_data\n","        \n","    def __len__(self):\n","        return len(self.img_data)\n","    \n","    def __getitem__(self, index):\n","        img_name = self.img_data.loc[index, 'Images']\n","        image = Image.open(img_name)\n","        image = image.convert('RGB')\n","\n","        gray = image.convert('L')\n","        gray_image = gray.convert('RGB')\n","        \n","        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n","        \n","        if self.transform is not None:\n","            image = self.transform(image)\n","            gray_image = self.transform(gray_image)\n","\n","        return image, gray_image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmYSBX6NVNlP"},"outputs":[],"source":["animal_dataset = Animals_Dataset(data,transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0CtvKy8WxjI"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(animal_dataset, batch_size=batch_size, \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(animal_dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler)\n"]},{"cell_type":"markdown","metadata":{"id":"7IzxElxbW2wG"},"source":["##Visualization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcVVnMYRW5RK"},"outputs":[],"source":["def img_display(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    npimg = np.transpose(npimg, (1, 2, 0))\n","    return npimg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0-c6NyfW6Fw"},"outputs":[],"source":["# get some random training images\n","#dataiter = iter(train_loader)\n","#images, labels = dataiter.next()\n","images,_, labels = next(iter(validation_loader))\n","animal_types = {0: 'butterfly', 1: 'cat', 2: 'cow', 3: 'dog', 4: 'elephant', 5: 'hen', 6: 'horse', 7: 'mokey', 8: 'panda', 9: 'sheep', 10: 'spider', 11: 'squirrel'}\n","\n","# Viewing data examples used for training\n","fig, axis = plt.subplots(3, 5, figsize=(10, 15))\n","for i, ax in enumerate(axis.flat):\n","    with torch.no_grad():\n","        image, label = images[i], labels[i]\n","        ax.imshow(img_display(image)) # add image\n","        ax.set(title = f\"{animal_types[label.item()]}\") # add label"]},{"cell_type":"markdown","metadata":{"id":"oH0Y6GIBNF8i"},"source":["## Color Image Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":953,"status":"ok","timestamp":1675705097448,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"OSfOO6bp7F2a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31312660-9c26-4acd-8ed7-b66cfb3a34c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training device: cuda\n"]}],"source":["# Device configuration\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Training device: {device}\")"]},{"cell_type":"code","source":["from torchvision import models, transforms\n","\n","color_model = models.vgg16(pretrained=True);\n","\n","color_model.classifier[6] = nn.Linear(in_features=4096, out_features=12);\n","\n","color_model.to(device);\n","color_model.train();\n"],"metadata":{"id":"RmBYEKfAMErS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVGz9tw8dhNM"},"outputs":[],"source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(color_model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(train_loader)"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"MWmAA9TXzSPP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHLGvjiTpqUV"},"outputs":[],"source":["n_epochs = 1\n","print_every = 25\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc = []\n","train_loss = []\n","train_acc = []\n","total_step = len(train_loader)\n","\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","    # scheduler.step(epoch)\n","    correct = 0\n","    total=0\n","    print(f'Epoch {epoch}\\n')\n","\n","    for i, (images,_, labels) in tqdm(enumerate(train_loader), total = len(train_loader)):\n","\n","        # Move tensors to the configured device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = color_model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        _,pred = torch.max(outputs, dim=1)\n","        correct += torch.sum(pred==labels).item()\n","        total += labels.size(0)\n","\n","        if (i) % print_every == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, i, total_step, loss.item()))\n","            \n","    train_acc.append(100 * correct / total)\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","\n","    batch_loss = 0\n","    total_t=0\n","    correct_t=0\n","\n","\n","    #validation\n","    with torch.no_grad():\n","        color_model.eval()\n","\n","        for (images,_,labels) in tqdm(validation_loader,total = len(validation_loader)):\n","\n","          #Move tensors to the configured device\n","          images = images.to(device)\n","          labels = labels.to(device)\n","\n","          outputs = color_model(images)\n","          loss = criterion(outputs, labels)\n","\n","          batch_loss += loss.item()\n","          _,pred_t = torch.max(outputs, dim=1)\n","          correct_t += torch.sum(pred_t==labels).item()\n","          total_t += labels.size(0)\n","\n","        val_acc.append(100 * correct_t / total_t)\n","        val_loss.append(batch_loss/len(validation_loader))\n","        network_learned = batch_loss < valid_loss_min\n","        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n","\n","\n","        # Saving the best weight \n","        if network_learned:\n","            valid_loss_min = batch_loss\n","            torch.save(color_model.state_dict(), 'vgg16-color.pt')\n","            print('Detected network improvement, saving current model')\n","\n","    color_model.train()"]},{"cell_type":"code","source":["# mount it\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"THKBrzW8YK0G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675706564206,"user_tz":-60,"elapsed":17342,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"95bd038a-5941-471f-b300-7b30e7c30d85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# copy it there\n","!cp /content/vgg16-color.pt /content/drive/MyDrive/TrainedNets"],"metadata":{"id":"vMMejyeFYIcj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grayscale Image Classification Training"],"metadata":{"id":"AAmIp9oFu6q8"}},{"cell_type":"code","source":["\n","gray_model = models.vgg16(pretrained=True);\n","\n","gray_model.classifier[6] = nn.Linear(in_features=4096, out_features=12);\n","\n","gray_model.to(device);\n","gray_model.train();\n"],"metadata":{"id":"BefZ6ud-u5HQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(gray_model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(train_loader)"],"metadata":{"id":"mfASasGlvNql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs = 3\n","print_every = 25\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc = []\n","train_loss = []\n","train_acc = []\n","total_step = len(train_loader)\n","\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","    # scheduler.step(epoch)\n","    correct = 0\n","    total=0\n","    print(f'Epoch {epoch}\\n')\n","\n","    for i, ( _, gray_img, labels) in tqdm(enumerate(train_loader), total = len(train_loader)):\n","\n","        # Move tensors to the configured device\n","        gray_img = gray_img.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = gray_model(gray_img)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        _,pred = torch.max(outputs, dim=1)\n","        correct += torch.sum(pred==labels).item()\n","        total += labels.size(0)\n","\n","        if (i) % print_every == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, i, total_step, loss.item()))\n","            \n","    train_acc.append(100 * correct / total)\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","\n","    batch_loss = 0\n","    total_t=0\n","    correct_t=0\n","\n","\n","    #validation\n","    with torch.no_grad():\n","        gray_model.eval()\n","\n","        for ( _, gray_img ,labels) in tqdm(validation_loader,total = len(validation_loader)):\n","\n","          #Move tensors to the configured device\n","          gray_img = gray_img.to(device)\n","          labels = labels.to(device)\n","\n","          outputs = gray_model(gray_img)\n","          loss = criterion(outputs, labels)\n","\n","          batch_loss += loss.item()\n","          _,pred_t = torch.max(outputs, dim=1)\n","          correct_t += torch.sum(pred_t==labels).item()\n","          total_t += labels.size(0)\n","\n","        val_acc.append(100 * correct_t / total_t)\n","        val_loss.append(batch_loss/len(validation_loader))\n","        network_learned = batch_loss < valid_loss_min\n","        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n","\n","\n","        # Saving the best weight \n","        if network_learned:\n","            valid_loss_min = batch_loss\n","            torch.save(gray_model.state_dict(), 'vgg16-gray.pt')\n","            print('Detected network improvement, saving current model')\n","\n","    gray_model.train()"],"metadata":{"id":"2RDhs6vlvOy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/vgg16-gray.pt /content/drive/MyDrive/TrainedNets"],"metadata":{"id":"2-_S57Z41Euq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1WzLFjlFqK9OzJyebQoC0yrCnDnszgYFR","timestamp":1675704989594}],"authorship_tag":"ABX9TyNV9G3VhBam086gcCaw6PB/"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}