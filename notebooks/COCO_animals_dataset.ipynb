{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook has been used for creating the datasets (the .txt files)**"
      ],
      "metadata": {
        "id": "gtB2tL2NniZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FASTAI"
      ],
      "metadata": {
        "id": "XOskLNIOELY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==2.4"
      ],
      "metadata": {
        "id": "9NlgDyYfEQc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COCO DOWNLOAD"
      ],
      "metadata": {
        "id": "4ud3_JOnEZZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.data.external import untar_data, URLs"
      ],
      "metadata": {
        "id": "ofvy7T2cEVFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_path = untar_data(URLs.COCO_SAMPLE)\n",
        "coco_path = str(coco_path) + \"/train_sample\""
      ],
      "metadata": {
        "id": "gs1hThWnEiLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "08b14de9-2522-4a57-e3a6-9eadaf63b0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "o1vG-DFuxVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = glob.glob(coco_path+\"/*.jpg\")\n",
        "paths =np.array(paths)\n",
        "num_images = len(paths)\n",
        "print(f\"# images: {num_images}\")"
      ],
      "metadata": {
        "id": "yAxG5GyOE7UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0d76ec-1eee-4f5b-d22d-782ae9b05064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# images: 21837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN dataset\n",
        "\n",
        "n_small = 6800\n",
        "n_big = 14800\n",
        "\n",
        "idxs = np.random.permutation(num_images)\n",
        "\n",
        "train_idxs_small = idxs[:n_small]\n",
        "train_idxs_big = idxs[:n_big]\n",
        "\n",
        "train_paths_small = paths[train_idxs_small]\n",
        "train_paths_big = paths[train_idxs_big]\n",
        "\n",
        "print(f\"Small dataset : {len(train_paths_small)} images,  Big dataset : {len(train_paths_big)} images\")"
      ],
      "metadata": {
        "id": "2U5BuED6w26W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b07315-6bc1-4258-d089-dd902ce57cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small dataset : 6800 images,  Big dataset : 14800 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for ax, img_path in zip(axes.flatten(), train_paths_big):\n",
        "    ax.imshow(Image.open(img_path))\n",
        "    ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "fIka3ZlC0RDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the .txt files"
      ],
      "metadata": {
        "id": "TSDYNQNKzZsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths_small = train_paths_small.tolist()\n",
        "train_paths_big = train_paths_big.tolist()\n",
        "\n",
        "with open('coco_small_training.txt', 'w') as f:\n",
        "  for line in train_paths_small:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "with open('coco_big_training.txt', 'w') as f:\n",
        "  for line in train_paths_big:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "tgHesemAzY3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for reading the files"
      ],
      "metadata": {
        "id": "td0mhnE_1ELh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename_small = \"coco_small_training.txt\"\n",
        "\n",
        "def read_lines(path):\n",
        "\n",
        "  lines = None\n",
        "\n",
        "  with open(path) as file:\n",
        "    lines = [line.rstrip() for line in file]\n",
        "\n",
        "  return lines\n",
        "\n",
        "tr_small_paths = read_lines(filename_small)\n",
        "print(f\"{len(tr_small_paths)} images\")\n",
        "\n",
        "\n",
        "tr_big_paths = read_lines(\"coco_big_training.txt\")\n",
        "print(f\"{len(tr_big_paths)} images\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gKHCf1Ug1Iw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to get the path for a validation/test dataset\n",
        "\n",
        "Execute the cells before"
      ],
      "metadata": {
        "id": "hXr6ENXO2cl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_small_paths = []\n",
        "test_big_paths = []\n",
        "\n",
        "paths = glob.glob(coco_path+\"/*.jpg\")\n",
        "\n",
        "for path in paths:\n",
        "\n",
        "  if path not in tr_big_paths:\n",
        "    test_big_paths.append(path)\n",
        "\n",
        "for path in paths:\n",
        "\n",
        "  if path not in tr_small_paths:\n",
        "    test_small_paths.append(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "HSfYZ4SS2k51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_small_paths), len(test_big_paths))"
      ],
      "metadata": {
        "id": "V5Fnm1pY46Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download animals dataset"
      ],
      "metadata": {
        "id": "CYf6lK5a7LSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "wfB7phlu7OfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"
      ],
      "metadata": {
        "id": "EsZZ5KIh7TTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "e3Hd3IUa7QWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "id": "tVRYRR-57ZNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"
      ],
      "metadata": {
        "id": "f_AIOp2d7fEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/animal_data\n",
        "!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"
      ],
      "metadata": {
        "id": "h2zrclKB7h1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to split it on two: training dataset (used to train the classifier or for the adversarial training) and validation/test dataset. Before you have to upload \"val_animals.txt\" that allows to split the dataset (two list with the paths will be obtained)"
      ],
      "metadata": {
        "id": "XhKHEFMt7lj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload();"
      ],
      "metadata": {
        "id": "LokHBUTH7kWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#set to None to use all the images (14K)\n",
        "max_img_per_class = 100\n",
        "\n",
        "path = \"/content/animal_data\"\n",
        "animal_path = path + \"/Animal Image Dataset\"\n",
        "\n",
        "animals = [\"butterfly\", \"cats\", \"cow\", \"dogs\", \"elephant\", \"hen\", \"horse\", \"monkey\", \"panda\", \"sheep\", \"spider\", \"squirrel\"]\n",
        "\n",
        "tr_animal_paths = []\n",
        "\n",
        "\n",
        "val_paths = []\n",
        "\n",
        "#collect paths validation/test images\n",
        "with open(\"/content/val_animals.txt\") as file:\n",
        "    val_paths = [line.rstrip() for line in file]\n",
        "\n",
        "\n",
        "\n",
        "for animal in animals:\n",
        "\n",
        "  counter = 0\n",
        "  folder = os.listdir(animal_path+\"/\"+animal)\n",
        "\n",
        "  for image in folder:\n",
        "\n",
        "    if counter == max_img_per_class:\n",
        "\n",
        "        break\n",
        "\n",
        "    if animal_path+\"/\"+animal+\"/\"+image not in val_paths:\n",
        "\n",
        "      tr_animal_paths.append(animal_path+\"/\"+animal+\"/\"+image)\n",
        "      counter +=1\n",
        "\n",
        "print(len(val_paths))\n",
        "print(len(tr_animal_paths))\n"
      ],
      "metadata": {
        "id": "nTPgEeKM8kb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a unique file .txt with all the training images (coco + animals)"
      ],
      "metadata": {
        "id": "A4HS7jyk-FAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_small = False\n",
        "\n",
        "coco_paths = tr_small_paths if use_small else tr_big_paths\n",
        "\n",
        "training_paths = tr_animal_paths + coco_paths\n",
        "\n",
        "#shuffle\n",
        "numpy_paths = np.array(training_paths)\n",
        "np.random.shuffle(numpy_paths)\n",
        "\n",
        "\n",
        "training_paths = numpy_paths.tolist()\n",
        "filename = \"coco_animals_big_training.txt\" if not use_small else \"coco_animals_small_training.txt\"\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "  for line in training_paths:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "jdT1383i-TW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = []\n",
        "\n",
        "test = read_lines(\"coco_animals_big_training.txt\")\n",
        "print(len(test))"
      ],
      "metadata": {
        "id": "NxszfQLHBcSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader"
      ],
      "metadata": {
        "id": "dcGtid1F7PjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n",
        "            ])\n",
        "\n",
        "\n",
        "class FromGrayToColorDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, transform = None):\n",
        "    \n",
        "    self.size = SIZE\n",
        "    self.paths = paths\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_rgb = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "    img_rgb = self.transform(img_rgb)\n",
        "    img_rgb = np.array(img_rgb)\n",
        "\n",
        "    #RGB -> Lab\n",
        "    img_lab = rgb2lab(img_rgb).astype(\"float32\")\n",
        "    img_lab = transforms.ToTensor()(img_lab)\n",
        "\n",
        "    #to have values in range [-1,1]\n",
        "    L = img_lab[0,:]/50. - 1.\n",
        "    ab = img_lab[[1,2],:] / 110.\n",
        "\n",
        "    return (L,ab)\n"
      ],
      "metadata": {
        "id": "qG9bE9XT2z34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FromGrayToColorDataset(train_paths, train_transform)\n",
        "test_dataset = FromGrayToColorDataset(test_paths, test_transform)"
      ],
      "metadata": {
        "id": "O0hbb3C--OUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PIN_MEMORY = True\n",
        "N_WORKERS = 2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n",
        "                            pin_memory=PIN_MEMORY, shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n",
        "                            pin_memory=PIN_MEMORY)"
      ],
      "metadata": {
        "id": "Hfw7ufUwPBQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "JQqbeRv8AU2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lab_to_rgb(L, ab):\n",
        "\n",
        "  \"\"\"\n",
        "  Provided a Lab image or a batch of Lab images, it returns it/them in RGB format \n",
        "  input:\n",
        "    - L: torch.tensor\n",
        "    - ab: torch.tensor\n",
        "  \n",
        "  output:\n",
        "    - img: numpy.ndarray (the rgb images)\n",
        "  \"\"\"\n",
        "\n",
        "  #check shape (one image or a batch)\n",
        "\n",
        "  is_batch = len(ab.shape) > 3\n",
        "  \n",
        "  L = (L+1.)*50.\n",
        "  ab = ab*110.\n",
        "\n",
        "  if is_batch:\n",
        "    # input tensors: N x 1 x 256 x 256, N x 2 x 256 x 256\n",
        "    Lab_images = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "  else:\n",
        "    # input tensors: 1 x 256 x 256, 2 x 256 x 256\n",
        "    Lab_image = torch.cat([L, ab], dim=0).permute(1, 2, 0).cpu().numpy()\n",
        "    return lab2rgb(Lab_image)\n",
        "\n",
        "  rgb_images = list()\n",
        "\n",
        "  for image in Lab_images:\n",
        "\n",
        "    img_rgb = lab2rgb(image)\n",
        "    rgb_images.append(img_rgb)\n",
        "\n",
        "  return np.stack(rgb_images, axis=0)\n",
        "\n",
        "L,ab = train_dataset[0]\n",
        "L2, ab2 = train_dataset[1]\n",
        "\n",
        "img1 = convert_lab_to_rgb(L, ab)\n",
        "print(img1.shape)\n",
        "L = torch.cat([L.unsqueeze(0).unsqueeze(0),L2.unsqueeze(0).unsqueeze(0)])\n",
        "ab = torch.cat([ab.unsqueeze(0),ab2.unsqueeze(0)])\n",
        "\n",
        "img2 = convert_lab_to_rgb(L, ab)\n",
        "print(img2.shape)\n",
        "\n",
        "img3 = img2[0]\n",
        "img4 = img2[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vpy48RgAWF4",
        "outputId": "9475f5f6-530c-41ec-dcdf-f47b1853a267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3)\n",
            "(2, 256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax1 = plt.subplot(1, 3, 1)\n",
        "ax1.imshow(img1)\n",
        "ax1.axis(\"off\")\n",
        "ax2 = plt.subplot(1, 3, 2)\n",
        "ax2.imshow(img3)\n",
        "ax2.axis(\"off\")\n",
        "ax3 = plt.subplot(1, 3, 3)\n",
        "ax3.imshow(img4)\n",
        "ax3.axis(\"off\")"
      ],
      "metadata": {
        "id": "RbjLlurVCGJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(batch[0].shape)\n",
        "print(batch[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj23zSxYIs-_",
        "outputId": "ee25f7c5-62c2-492e-a8de-84ef16f38b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 256, 256])\n",
            "torch.Size([32, 2, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(Ls, abs, n_cols = 4):\n",
        "\n",
        "  \"\"\"\n",
        "  provided a batch of images, visualize them\n",
        "  input:\n",
        "    - Ls: batch with L for each image, N x 1 x 256 x 256 tensor\n",
        "    - abs: batch with ab for each image, N x 2 x 256 x 256 tensor\n",
        "  \"\"\"\n",
        "  batch_size = Ls.shape[0]\n",
        "  num_rows = batch_size//n_cols\n",
        "\n",
        "  rgb_images = convert_lab_to_rgb(Ls, abs)\n",
        "\n",
        "  \n",
        "  fig = plt.figure(figsize=(10, 20))\n",
        "\n",
        "  for i in range(num_rows):\n",
        "\n",
        "    for j in range(n_cols):\n",
        "      ax = plt.subplot(num_rows, n_cols, i*n_cols + j +1)\n",
        "      ax.imshow(rgb_images[i*n_cols+j])\n",
        "      ax.axis(\"off\")\n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "show_batch(batch[0].unsqueeze(1), batch[1])"
      ],
      "metadata": {
        "id": "zhecf4JGPv_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(Ls, real_abs, fake_abs):\n",
        "\n",
        "  \"\"\"\n",
        "  provided a batch of real and fake images, visualize them (+ the gray image)\n",
        "  input:\n",
        "    - Ls: batch with L for each image, N x 1 x 256 x 256 tensor\n",
        "    - real_abs: batch with ab for each real image, N x 2 x 256 x 256 tensor\n",
        "    - fake_abs: batch with ab for each fake image, N x 2 x 256 x 256 tensor\n",
        "  \"\"\"\n",
        "\n",
        "  n_cols = Ls.shape[0]\n",
        "\n",
        "  real_images = convert_lab_to_rgb(Ls, real_abs)\n",
        "  fake_images = convert_lab_to_rgb(Ls, fake_abs)\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(n_cols):\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1)\n",
        "    ax.imshow(Ls[i][0].cpu(), cmap='gray')\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1+n_cols)\n",
        "    ax.imshow(real_images[i])\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1+2*n_cols)\n",
        "    ax.imshow(fake_images[i])\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "show_results(batch[0].unsqueeze(1)[:4], batch[1][:4], batch[1][:4])"
      ],
      "metadata": {
        "id": "3nxCsJUNTvTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}