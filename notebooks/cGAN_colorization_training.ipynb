{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook have been used for the cGAN training**\n"
      ],
      "metadata": {
        "id": "CceyNaDfg6Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z-oYqlb5h2uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e54QtGM-4FGH"
      },
      "source": [
        "# 1 Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk3EcePY4PV1"
      },
      "source": [
        "Two datasets are used: a small version of COCO dataset with 21,837 images and one with 17,178 images of animals (12 categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B0UnTf94ndc"
      },
      "source": [
        "##1.1 Animals dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbIXQR5B5Qty"
      },
      "source": [
        "We download this dataset from kaggle (1.4 GB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5LyAJsIy4H9R"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW1I4x0j5dLh"
      },
      "source": [
        "You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcwM9NAY5cJQ"
      },
      "outputs": [],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgTBTr6f5yv8"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X-z45AP54t6"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep7psYmO6Dmv"
      },
      "source": [
        "The data have been downloaded. To unzip them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ldwePf3c59cJ"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/animal_data\n",
        "!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sILCvtJ8rbC"
      },
      "source": [
        "## 1.2 COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grBxE-GZ8v1g"
      },
      "source": [
        "To download it we use fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of1FTMIW8VXG"
      },
      "outputs": [],
      "source": [
        "!pip install fastai==2.4;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PGmlvToL83YY"
      },
      "outputs": [],
      "source": [
        "from fastai.data.external import untar_data, URLs\n",
        "import os\n",
        "import glob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oQGGmVW83uq"
      },
      "outputs": [],
      "source": [
        "coco_path = untar_data(URLs.COCO_SAMPLE)\n",
        "coco_path = str(coco_path) + \"/train_sample\"\n",
        "\n",
        "paths = glob.glob(coco_path+\"/*.jpg\")\n",
        "paths =np.array(paths)\n",
        "num_images_coco = len(paths)\n",
        "print(f\"# coco images: {num_images_coco}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplaod either data_small_training.txt or data_big_training.txt"
      ],
      "metadata": {
        "id": "PDax1udw8PRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload();"
      ],
      "metadata": {
        "id": "WhULxpYT8Sla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"data_small_training.txt\"\n",
        "\n",
        "def read_lines(path):\n",
        "\n",
        "  lines = None\n",
        "\n",
        "  with open(path) as file:\n",
        "    lines = [line.rstrip() for line in file]\n",
        "\n",
        "  return lines"
      ],
      "metadata": {
        "id": "6D-j-Gmw8jpP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_paths = read_lines(filename)\n",
        "print(f\"{len(training_paths)} images for training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kkkSVU8sUl",
        "outputId": "2cd7e5ce-0758-4bd0-be12-ab198abeb912"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9600 images for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LOVOSm0b9QU"
      },
      "source": [
        "# 2 Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_w-5Nc7mgY38"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya73EyyAkVV3"
      },
      "source": [
        "## 2.1 Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YyNuqjo6peDz"
      },
      "outputs": [],
      "source": [
        "SIZE = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UtCJVViBrLMK"
      },
      "outputs": [],
      "source": [
        "class GrayToColorDataset(Dataset):\n",
        "\n",
        "  def __init__(self, paths, transform = None):\n",
        "    \n",
        "    self.paths = paths\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_rgb = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "    img_rgb = self.transform(img_rgb)\n",
        "    img_rgb = np.array(img_rgb)\n",
        "\n",
        "    #RGB -> Lab\n",
        "    img_lab = rgb2lab(img_rgb).astype(\"float32\")\n",
        "    img_lab = transforms.ToTensor()(img_lab)\n",
        "\n",
        "    #to have values in range [-1,1]\n",
        "    L = img_lab[[0],:]/50. - 1.\n",
        "    ab = img_lab[[1,2],:] / 110.\n",
        "\n",
        "    return (L,ab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rwCDspxZrYlp"
      },
      "outputs": [],
      "source": [
        "train_dataset = GrayToColorDataset(training_paths, train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "222dWNwbrpeJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "PIN_MEMORY = True\n",
        "N_WORKERS = 2\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n",
        "                            pin_memory=PIN_MEMORY, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNsZnfNfsiBg"
      },
      "source": [
        "# 3 cGAN models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZkM2-UhsvBp"
      },
      "source": [
        "## 3.1 Generator: U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Vnxybgnsr2Y"
      },
      "outputs": [],
      "source": [
        "class UNetDown(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size = 4, normalization_type = None, dropout = 0.0, activation = None):\n",
        "\n",
        "    super(UNetDown, self).__init__()\n",
        "\n",
        "    #if batchnorm/instancenorm used, bias not used\n",
        "\n",
        "    use_bias = normalization_type == None\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n",
        "\n",
        "    if not use_bias:\n",
        "      \n",
        "      if normalization_type == \"instance\":\n",
        "\n",
        "        layers.append(nn.InstanceNorm2d(out_channels))\n",
        "\n",
        "      else:\n",
        "\n",
        "        layers.append( nn.BatchNorm2d(out_channels))\n",
        "        \n",
        "    if activation == None:\n",
        "      layers.append(nn.LeakyReLU(negative_slope = 0.2))\n",
        "\n",
        "    if activation == \"ReLU\":\n",
        "\n",
        "      layers.append(nn.ReLU())\n",
        "\n",
        "    if dropout:\n",
        "\n",
        "      layers.append(nn.Dropout(p = dropout))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QqTa1lzVtnwT"
      },
      "outputs": [],
      "source": [
        "class UNetUp(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size = 4,  normalization_type = None, dropout = 0.0):\n",
        "\n",
        "    super(UNetUp, self).__init__()\n",
        "\n",
        "    use_bias = normalization_type == None\n",
        "\n",
        "    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n",
        "\n",
        "    if not use_bias:\n",
        "      if normalization_type == \"instance\":\n",
        "\n",
        "        layers.append(nn.InstanceNorm2d(out_channels))\n",
        "\n",
        "      else:\n",
        "\n",
        "        layers.append( nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    layers.append(nn.ReLU())\n",
        "\n",
        "    if dropout:\n",
        "\n",
        "      layers.append(nn.Dropout(p = dropout))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x, skip = None):\n",
        "      x = self.model(x)\n",
        "      if skip is not None:\n",
        "\n",
        "        x = torch.cat((skip, x), 1)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4UQKVsqrtqtM"
      },
      "outputs": [],
      "source": [
        "class GeneratorUNet(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels = 1, out_channels = 2, num_down = 8, ngf = 64, normalization_type = None):\n",
        "\n",
        "    super(GeneratorUNet, self).__init__()\n",
        "\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "    \n",
        "\n",
        "    features =[ngf]\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "      features.append(features[i]*2)\n",
        "\n",
        "    features.append(features[-1])\n",
        "    #64, 128, 256, 512, 512\n",
        "\n",
        "    if num_down > 5:\n",
        "\n",
        "      features += [ngf * 8 for i in range(num_down - 5)]\n",
        "    #for num_down = 8: 64, 128, 256, 512, 512, 512, 512, 512 (->1x1 for input size 256x256)\n",
        "\n",
        "\n",
        "    #ENCODER (CONTRACTING PATH)\n",
        "\n",
        "    #outermost down block: no normalization and no dropout, only downconv\n",
        "    self.downs.append(UNetDown(in_channels, ngf, 4))\n",
        "\n",
        "    in_channels = ngf #new in_channels for the next down-block\n",
        "    \n",
        "    for i,n_features in enumerate(features[1:len(features)-1]):\n",
        "      #no dropout\n",
        "      self.downs.append(UNetDown(in_channels, n_features, 4, normalization_type, 0.0))\n",
        "      in_channels = n_features\n",
        "\n",
        "    \n",
        "    #innermost down block: no normalization and no dropout, only downconv\n",
        "    self.downs.append(UNetDown(in_channels, features[-1], 4, activation = \"ReLU\"))\n",
        "    \n",
        "\n",
        "    #DECODER (EXPANSIVE PATH)\n",
        "    i_channels = in_channels\n",
        "    for i, n_features in enumerate((features[-2::-1])):\n",
        "      \n",
        "      \n",
        "      #if i == 0, innermost(bottleneck), namely a block such that after down we go up. no dropout\n",
        "      i_channels = in_channels if i == 0  else i_channels * 2\n",
        "\n",
        "      #no dropout for the first up and the last 4 ups \n",
        "      dropout = 0.0 if (i == 0 or i  > 3) else 0.5\n",
        "\n",
        "      self.ups.append(UNetUp(i_channels, n_features, 4, normalization_type, dropout))\n",
        "      i_channels = n_features\n",
        "    \n",
        "    \n",
        "    self.final = nn.Sequential(\n",
        "        nn.ConvTranspose2d(ngf*2,out_channels, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    skip_connections = list()\n",
        "\n",
        "    #encoder\n",
        "    for down in self.downs:\n",
        "\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "\n",
        "    #decoder with skip connections\n",
        "    for i, up in enumerate(self.ups):\n",
        "      \n",
        "      x = up(x, skip_connections[-i-2])\n",
        "\n",
        "    return self.final(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EakwAjpstItC"
      },
      "source": [
        "## 3.2 Discrimintor: PatchGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfSxSbH5-fle"
      },
      "source": [
        "The descriminator is a PatchGAN for $N \\times N$ patches where $N=70$: given an input $256 \\times 256$ the output is $30 \\times 30$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gIK5GgDytMoi"
      },
      "outputs": [],
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels = 3, ndf = 64, n_down = 5, normalization_type = \"batchnorm\"):\n",
        "\n",
        "    super(PatchDiscriminator, self).__init__()\n",
        "\n",
        "    features = [ndf * 2**i for i in range(n_down-1)]\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for i in range(len(features)):\n",
        "      use_bias = True if i < 1  else False\n",
        "      stride = 2 if i < (len(features)-1) else 1\n",
        "      layers.append(nn.Conv2d(in_channels, features[i], 4, stride, 1, bias = use_bias))\n",
        "\n",
        "      if not use_bias:\n",
        "        if normalization_type == \"batchnorm\":\n",
        "          layers.append(nn.BatchNorm2d(features[i]))\n",
        "          \n",
        "        if normalization_type == \"instance\":\n",
        "          layers.append(nn.InstanceNorm2d(features[i]))\n",
        "\n",
        "\n",
        "      layers.append(nn.LeakyReLU(0.2))\n",
        "\n",
        "      in_channels = features[i]\n",
        "    \n",
        "    layers.append(nn.Conv2d(in_channels, 1, 4, 1, 1))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ8ki3pfyrz5"
      },
      "source": [
        "# 4 GAN LOSS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6gdCkc0Bamk"
      },
      "source": [
        "The following class allows to implement the GAN loss: for the discriminator \n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{x,y}[\\log D(x,y)]+\\mathbb{E}_{x,z}[\\log(1-D(x,G(z, x)))]\n",
        "\\end{equation}\n",
        "\n",
        "For the generator instead\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{x,z}[\\log D(x,G(z,x))]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ya2IZzYjyRg0"
      },
      "outputs": [],
      "source": [
        "class GANLoss():\n",
        "\n",
        "  def __init__(self, device):\n",
        "\n",
        "    self.criteria = nn.BCEWithLogitsLoss()\n",
        "    self.real = 1.\n",
        "    self.fake = 0.\n",
        "    self.device = device\n",
        "\n",
        "  def __call__(self, input, label_type):\n",
        "    \n",
        "    label = torch.tensor(self.real if label_type else self.fake)\n",
        "    \n",
        "    labels = label.expand_as(input).to(self.device)\n",
        "    \n",
        "    return self.criteria(input, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Vf25pPUZJR"
      },
      "source": [
        "# 5 Models initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vdfiGrcUl-p"
      },
      "source": [
        "Initialization generator and discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joKhtB9AUpdr"
      },
      "outputs": [],
      "source": [
        "#G initialized Generator\n",
        "#D initialized Discriminator\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "G = GeneratorUNet(1,2,8,64, \"batchnorm\").to(device)\n",
        "#G = Unet(input_c=1, output_c=2, n_down=8, num_filters=64).to(device)\n",
        "D = PatchDiscriminator().to(device)\n",
        "\n",
        "def weights_init(m):\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        #nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0.0)\n",
        "    \n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "G.apply(weights_init);\n",
        "D.apply(weights_init);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G64C5Nv9NSmx"
      },
      "source": [
        "# 6 Training setup\n",
        "\n",
        "Here we define the losses (GAN loss and L1), the optimizer, the number of epochs and the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1fteHZMOkD8"
      },
      "source": [
        "## 6.1 Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qLQtSVlwPNnf"
      },
      "outputs": [],
      "source": [
        "GAN_loss = GANLoss(device) \n",
        "L1_loss = nn.L1Loss()\n",
        "\n",
        "#L1 hyperparam\n",
        "\n",
        "lamb = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5qyRPWGPpPD"
      },
      "source": [
        "## 6.2 Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kC4Vqh1jNxIB"
      },
      "outputs": [],
      "source": [
        "#params for Adam\n",
        "lr_G = 2e-4 \n",
        "lr_D = 2e-4\n",
        "\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "G_opt = optim.Adam(G.parameters(), lr=lr_G, betas=betas)\n",
        "D_opt = optim.Adam(D.parameters(), lr=lr_D, betas=betas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptf8P0-HQR0f"
      },
      "source": [
        "# 7 Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JV9IHLHdQU0N"
      },
      "outputs": [],
      "source": [
        "def convert_lab_to_rgb(L, ab):\n",
        "\n",
        "  \"\"\"\n",
        "  Provided a Lab image or a batch of Lab images, it returns it/them in RGB format \n",
        "  input:\n",
        "    - L: torch.tensor\n",
        "    - ab: torch.tensor\n",
        "  \n",
        "  output:\n",
        "    - img: numpy.ndarray (the rgb images)\n",
        "  \"\"\"\n",
        "\n",
        "  #check shape (one image or a batch)\n",
        "\n",
        "  is_batch = len(ab.shape) > 3\n",
        "  \n",
        "  L = (L+1.)*50.\n",
        "  ab = ab*110.\n",
        "\n",
        "  if is_batch:\n",
        "    # input tensors: N x 1 x 256 x 256, N x 2 x 256 x 256\n",
        "    Lab_images = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "  else:\n",
        "    # input tensors: 1 x 256 x 256, 2 x 256 x 256\n",
        "    Lab_image = torch.cat([L, ab], dim=0).permute(1, 2, 0).cpu().detach().numpy()\n",
        "    return lab2rgb(Lab_image)\n",
        "\n",
        "  rgb_images = list()\n",
        "\n",
        "  for image in Lab_images:\n",
        "\n",
        "    img_rgb = lab2rgb(image)\n",
        "    rgb_images.append(img_rgb)\n",
        "\n",
        "  return np.stack(rgb_images, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rBi11kGaWoiR"
      },
      "outputs": [],
      "source": [
        "def show_results(Ls, real_abs, fake_abs):\n",
        "\n",
        "  \"\"\"\n",
        "  provided a batch of real and fake images, visualize them (+ the gray images)\n",
        "  input:\n",
        "    - Ls: batch with L for each image, N x 1 x 256 x 256 tensor\n",
        "    - real_abs: batch with ab for each real image, N x 2 x 256 x 256 tensor\n",
        "    - fake_abs: batch with ab for each fake image, N x 2 x 256 x 256 tensor\n",
        "  \"\"\"\n",
        "\n",
        "  n_cols = Ls.shape[0]\n",
        "\n",
        "  real_images = convert_lab_to_rgb(Ls, real_abs)\n",
        "  fake_images = convert_lab_to_rgb(Ls, fake_abs)\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(n_cols):\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1)\n",
        "    ax.imshow(Ls[i][0].cpu(), cmap='gray')\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1+n_cols)\n",
        "    ax.imshow(real_images[i])\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(3, n_cols, i+1+2*n_cols)\n",
        "    ax.imshow(fake_images[i])\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class allows to track the losses over an epoch: it accumilates the losses over the dataset for each loss (GAN_loss for the generator, for the discriminator, etc.). Then allows to compute the mean of each losses. In this way we can compute the mean loss at each epoch and also in intermediate steps."
      ],
      "metadata": {
        "id": "oc5APyfh3_hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossTracker :\n",
        "  def __init__(self) :\n",
        "\n",
        "    self.count = 0 #n of images seen up to now\n",
        "    self.avg = {} #dict with avg of losses\n",
        "\n",
        "    self.dict_losses = {\n",
        "        \"G_loss\" : 0,\n",
        "        \"G_GAN_loss\" : 0,\n",
        "        \"G_L1_loss\" : 0,\n",
        "        \"D_loss\" : 0,\n",
        "        \"D_loss_real\" : 0,\n",
        "        \"D_loss_fake\" : 0\n",
        "    }\n",
        "\n",
        "  def set_to_zero(self) :\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    for key in self.dict_losses :\n",
        "      self.dict_losses[key] = 0\n",
        "    self.count = 0  \n",
        "\n",
        "\n",
        "  def update_losses(self, losses, batch_size) :\n",
        "    \"\"\"\n",
        "    It updates the cumulative sum for the losses\n",
        "\n",
        "    - losses: dict, the losses obtained in a single batch of dimension batch_size\n",
        "    - batch_size: int\n",
        "    \"\"\"\n",
        "    for key in losses :\n",
        "      self.dict_losses[key] += losses[key] * batch_size\n",
        "\n",
        "    self.count += batch_size\n",
        "\n",
        "\n",
        "  def avg_losses(self) : \n",
        "\n",
        "    \"\"\"\n",
        "     for each loss metric compute the mean w.r.t. the \n",
        "     losses accumulate up to now\n",
        "    \"\"\"\n",
        "\n",
        "    for key in self.dict_losses :\n",
        "      self.avg[key] = self.dict_losses[key] / self.count \n",
        "\n",
        "    return self.avg\n",
        "\n",
        "\n",
        "  def print_losses(self) :\n",
        "\n",
        "    \"\"\"\n",
        "    print of the mean loss for each loss metric\n",
        "    \"\"\"\n",
        "\n",
        "    for key in self.avg :\n",
        "      temp = self.avg[key]\n",
        "      print(f\"{key} : {temp}\")"
      ],
      "metadata": {
        "id": "xpwp15Iyx9P9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp4m5mkrdM3u"
      },
      "source": [
        "The following functions are used to save and load checkpoints during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tUXrAtbcavh4"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model_G, model_D, opt_G, opt_D, dict_losses):\n",
        "  \"\"\"\n",
        "  Provided the current epoch, G, D, optG, optD, and the losses, this function saves a checkpoint\n",
        "  \"\"\"\n",
        "\n",
        "  dict_save = {\n",
        "                'epoch': epoch,\n",
        "                'G_state_dict': model_G.state_dict(),\n",
        "                'G_opt_state_dict': opt_G.state_dict(),\n",
        "                'D_state_dict' : model_D.state_dict(),\n",
        "                'D_opt_state_dict' : opt_D.state_dict(),\n",
        "                'G_GAN_loss' : dict_losses['G_GAN_loss'],\n",
        "                'D_loss' : dict_losses['D_loss'],\n",
        "                'G_loss' : dict_losses['G_loss']\n",
        "              }\n",
        "\n",
        "  #dict_save.update(dict_losses)\n",
        "  #print(dict_save)\n",
        "\n",
        "  torch.save(dict_save, \"cGAN_training.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zgmbZ03LdkPw"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(G, D, opt_G, opt_D, path = \"/content/cGAN_training.pt\"):\n",
        "\n",
        "  \"\"\"\n",
        "  - G: Generator model, GeneratorUNet\n",
        "  - D: Discriminator model, PatchDiscriminator \n",
        "  - opt_G: optimizer for G\n",
        "  - opt_D: optimizer for D\n",
        "  \n",
        "  - path: path to the file from which load the checkpoint\n",
        "  \"\"\"\n",
        "\n",
        "  checkpoint = torch.load(path)\n",
        "  epoch = checkpoint['epoch']\n",
        "  G.load_state_dict(checkpoint['G_state_dict'])\n",
        "  D.load_state_dict(checkpoint['D_state_dict'])\n",
        "\n",
        "  opt_D.load_state_dict(checkpoint['D_opt_state_dict'])\n",
        " \n",
        "  opt_G.load_state_dict(checkpoint['G_opt_state_dict'])\n",
        "\n",
        "\n",
        "  print(f\"Checkpoint at epoch {epoch} loaded\")\n",
        "\n",
        "  return epoch, {'G_GAN_loss' : checkpoint['G_GAN_loss'], 'D_loss' : checkpoint['D_loss'], 'G_loss' : checkpoint['G_loss']}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to save and load only the generator"
      ],
      "metadata": {
        "id": "SUMKteS3jVO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_generator(G, path = \"/content/cGAN-gen.pt\"):\n",
        "\n",
        "  torch.save(G.state_dict(), path)\n",
        "\n",
        "def load_generator(G, path = \"/content/cGAN-gen.pt\"):\n",
        "\n",
        "  G.load_state_dict(torch.load(path))\n",
        "\n"
      ],
      "metadata": {
        "id": "DGQ3wlPsjZb4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and update a .csv file with all the losses during the training"
      ],
      "metadata": {
        "id": "8nL-1fBSogYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from csv import writer\n",
        "\n",
        "loss_names = ['G_loss', 'G_GAN_loss', 'G_L1_loss', 'D_loss_real', 'D_loss_fake', 'D_loss']\n",
        "\n",
        "def create_csv(columns = loss_names, path = \"/content/losses.csv\"):\n",
        "\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "  df.to_csv(path)\n",
        "\n",
        "def update_csv(epoch, losses, columns = loss_names, path = \"/content/losses.csv\"):\n",
        "\n",
        "  row_1 = [epoch]\n",
        "  row_2 = [losses[column] for column in columns]\n",
        "\n",
        "  row_to_add = row_1 + row_2\n",
        "\n",
        "  with open(path, 'a') as f_object:\n",
        "\n",
        "    writer_object = writer(f_object)\n",
        " \n",
        "    writer_object.writerow(row_to_add)\n",
        "\n",
        "    f_object.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "fAUDlSSTohCO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kQG-T2DQVEJ"
      },
      "source": [
        "# 8 Training\n",
        "\n",
        "Run the cell in 8.3 only if the training has been interrupted and you want to complete the 100 epochs (more details later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm_1wEoRm6sp"
      },
      "source": [
        "## 8.1 Functions for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaAyfddkY_rt"
      },
      "source": [
        "Define a training step. Pass the b&w image (L), the ab of the true image  and the others parameters needed. This function update both the parameters of the generator $G$ and the parameters of the discriminator $D$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "41TY16FJZBe5"
      },
      "outputs": [],
      "source": [
        "def train_step(L, ab_real, g, d, device, dis_opt, gen_opt):\n",
        "\n",
        "    g.train()\n",
        "    d.train()\n",
        "\n",
        "    #forward\n",
        "    ab_fake = g(L)\n",
        "\n",
        "    real_imgs = torch.cat([L, ab_real], dim = 1)\n",
        "    fake_imgs = torch.cat([L, ab_fake], dim = 1)\n",
        "\n",
        "    #UPDATE DISCRIMINATOR\n",
        "    dis_opt.zero_grad()\n",
        "\n",
        "    real_output = d(real_imgs)\n",
        "    fake_output = d(fake_imgs.detach())\n",
        "\n",
        "    D_loss_real = GAN_loss(real_output, True)\n",
        "    D_loss_fake = GAN_loss(fake_output, False)\n",
        "\n",
        "    D_loss = (D_loss_real + D_loss_fake) * 0.5 #???\n",
        "\n",
        "    D_loss.backward()\n",
        "    dis_opt.step()\n",
        "\n",
        "    #UPDATE GENERATOR\n",
        "    gen_opt.zero_grad()\n",
        "\n",
        "    fake_output = d(fake_imgs)\n",
        "    G_GAN_loss = GAN_loss(fake_output, True)\n",
        "    G_L1_loss = L1_loss(ab_fake, ab_real) * lamb\n",
        "\n",
        "    G_loss = G_L1_loss + G_GAN_loss\n",
        "\n",
        "    \n",
        "    G_loss.backward()\n",
        "    gen_opt.step()\n",
        "    \n",
        "    return {\"G_loss\" : G_loss.item(), \"G_GAN_loss\" : G_GAN_loss.item(),\"G_L1_loss\" : G_L1_loss.item(), \"D_loss\" : D_loss.item(), \"D_loss_real\" : D_loss_real.item(), \"D_loss_fake\" : D_loss_fake.item()}, ab_fake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following funtion implements the training over a number of epochs specified"
      ],
      "metadata": {
        "id": "t69LLpfB2O-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BE5tEN-abFDK"
      },
      "outputs": [],
      "source": [
        "import time as time\n",
        "\n",
        "def train(dataloader, epochs, g, d, device, dis_opt, gen_opt, print_every, last_epoch_done = None):\n",
        "\n",
        "  losstracker = LossTracker()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    losstracker.set_to_zero()\n",
        "    \n",
        "    count = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "\n",
        "        L = batch[0].to(device)\n",
        "        ab = batch[1].to(device)\n",
        "\n",
        "        #G_loss, D_loss, G_GAN_loss, G_L1_loss, D_loss_real, D_loss_fake, ab_fake = train_step(L, ab, g, d, device, dis_opt, gen_opt)\n",
        "        dict_losses, ab_fake = train_step(L, ab, g, d, device, dis_opt, gen_opt)\n",
        "        \n",
        "        #track losses at each epoch\n",
        "        losstracker.update_losses(dict_losses, L.shape[0])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        if (count % print_every == 0):\n",
        "            #PRINT avg losses\n",
        "            #show_results(L[:3], ab[:3], ab_fake[:3])\n",
        "            losstracker.avg_losses()\n",
        "            losstracker.print_losses()\n",
        "            print(\"\\n\")\n",
        "                       \n",
        "\n",
        "    #SAVING MODELS + OPTs\n",
        "    print(\"Saving model checkpoint\")\n",
        "    add = 0 if last_epoch_done == None else 1+last_epoch_done\n",
        "\n",
        "    #compute avg losses over the epoch\n",
        "    losses = losstracker.avg_losses()\n",
        "\n",
        "    #save\n",
        "    if epoch == (epochs -1):\n",
        "      save_generator(g)\n",
        "    else:\n",
        "      save_checkpoint(epoch+add, g, d, gen_opt, dis_opt, losses)\n",
        "\n",
        "\n",
        "    #save losses in csv\n",
        "\n",
        "    update_csv(epoch+add, losses)\n",
        "\n",
        "    print(f'Epoch {epoch+add} finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rho4nI-kbrvz"
      },
      "source": [
        "## 8.2 Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eImfCDrbuMd"
      },
      "outputs": [],
      "source": [
        "#training phase\n",
        "\n",
        "#create csv to save losses\n",
        "create_csv()\n",
        "\n",
        "EPOCHS = 100    \n",
        "print_every = 600\n",
        "train( train_dataloader, EPOCHS, G, D, device, D_opt, G_opt, print_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5PuT4unDUT"
      },
      "source": [
        "## 8.3 Resume training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case in which the training is interrupted, you can resume the training with this cell: firstly upload the last .pt saved during training ('cGAN_training.pt') on Colab (if you have changed either the name of the file or the path, '/content/', you just specify to 'load_checkpoint' as extra argument the path. Have a look to the definition of the function)"
      ],
      "metadata": {
        "id": "wmGdYd082X75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GiwZzChnGck"
      },
      "outputs": [],
      "source": [
        "#define the model from which to start\n",
        "EPOCHS = 100\n",
        "G = GeneratorUNet(1,2,8,64, \"batchnorm\").to(device)\n",
        "D = PatchDiscriminator().to(device) \n",
        " \n",
        "#G.apply(weights_init)\n",
        "#D.apply(weights_init)    \n",
        "                                \n",
        "G_opt = optim.Adam(G.parameters(), lr=lr_G, betas=betas)   \n",
        "D_opt = optim.Adam(D.parameters(), lr=lr_D, betas=betas)\n",
        "\n",
        "#if the training is stopped load the model and restart from the last point until #EPOCHS are reached\n",
        "epoch, check_point_losses = load_checkpoint(G, D, G_opt, D_opt)       \n",
        "\n",
        "print(check_point_losses)              \n",
        "\n",
        "epochs_left = EPOCHS - epoch  - 1   \n",
        "print_every = 600    \n",
        "  \n",
        "train( train_dataloader, epochs_left, G, D, device, D_opt, G_opt, print_every, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 Load and test Generator (to test properly the generator look at the other notebooks)"
      ],
      "metadata": {
        "id": "9ZdPplguOD1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells load the generator and show the results with some new images. First of all load a file with the paths for images of the animals dataset (not used during training). **Upload the \"test_animals.txt\"**"
      ],
      "metadata": {
        "id": "JdosKfULOKOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload();  "
      ],
      "metadata": {
        "id": "kc9ffSU3On5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"val_animals.txt\"\n",
        "\n",
        "def read_lines(path):\n",
        "\n",
        "  lines = None\n",
        "\n",
        "  with open(path) as file:\n",
        "    lines = [line.rstrip() for line in file]\n",
        "\n",
        "  return lines\n",
        "\n",
        "animals_paths = read_lines(filename)\n",
        "\n",
        "print(f\"# images of animals for testing: {len(animals_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2azYxO4PDVW",
        "outputId": "600241ee-2e02-4811-bd2a-1bd43dad1bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# images of animals for testing: 2400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the other images of COCO dataset"
      ],
      "metadata": {
        "id": "3xyTfsWlPibR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths = animals_paths\n",
        "\n",
        "for path in paths:\n",
        "  \n",
        "  if path not in training_paths:\n",
        "\n",
        "    test_paths.append(path)\n",
        "\n",
        "print(f\"# images for testing: {len(test_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxBUFkDqPoPM",
        "outputId": "96263921-363a-46fa-b4d3-88f853e0f6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# images for testing: 9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(33)\n",
        "import random\n",
        "\n",
        "random.seed(33)\n",
        "\n",
        "test_paths = np.array(test_paths)\n",
        "np.random.shuffle(test_paths)"
      ],
      "metadata": {
        "id": "X4p7PCK6QVqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset and dataloader"
      ],
      "metadata": {
        "id": "fcNOcFD_RQkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 256\n",
        "batch_size = 3\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n",
        "            ])\n",
        "\n",
        "test_dataset = GrayToColorDataset(test_paths, test_transform)\n",
        "\n",
        "PIN_MEMORY = True\n",
        "N_WORKERS = 2\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=N_WORKERS,\n",
        "                            pin_memory=PIN_MEMORY, shuffle = False)"
      ],
      "metadata": {
        "id": "OjFh79oiRMxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load generator"
      ],
      "metadata": {
        "id": "LAeeiClnRflJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = GeneratorUNet(1,2,8,64, \"batchnorm\").to(device);\n",
        "\n",
        "load_generator(G);\n",
        "\n",
        "#G.train(); #\n",
        "G.eval();"
      ],
      "metadata": {
        "id": "nNQe40vVRiyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results cicle"
      ],
      "metadata": {
        "id": "FaS7TwbkRzlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_generator(G, device, dataloader, n_batcher_to_show = 4):\n",
        "\n",
        "  for i,batch in enumerate(dataloader):\n",
        "        print(f\"Results for batch {i+1}\")\n",
        "        L = batch[0].to(device)\n",
        "        ab = batch[1].to(device)\n",
        "\n",
        "        ab_fake = G(L)\n",
        "\n",
        "        show_results(L, ab, ab_fake)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "        if i == n_batcher_to_show -1:\n",
        "          break\n",
        "\n",
        "test_generator(G, device, test_dataloader, 2)"
      ],
      "metadata": {
        "id": "GXuWb4nZRzBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}