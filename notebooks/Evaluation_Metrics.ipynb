{"cells":[{"cell_type":"markdown","metadata":{"id":"e54QtGM-4FGH"},"source":["# 1 Data collection"]},{"cell_type":"markdown","metadata":{"id":"Sk3EcePY4PV1"},"source":["Two datasets are used: a small version of COCO dataset with 21,837 images and one with 17,178 images of animals (12 categories)"]},{"cell_type":"markdown","metadata":{"id":"1B0UnTf94ndc"},"source":["##1.1 Animals dataset"]},{"cell_type":"markdown","metadata":{"id":"QbIXQR5B5Qty"},"source":["We download this dataset from kaggle (1.4 GB)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5LyAJsIy4H9R","executionInfo":{"status":"ok","timestamp":1675950412410,"user_tz":-60,"elapsed":5846,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["!pip install -q kaggle\n","from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"gW1I4x0j5dLh"},"source":["You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"elapsed":11295,"status":"ok","timestamp":1675950423696,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"HcwM9NAY5cJQ","outputId":"d57fc191-2c9e-4884-9faf-d35ec331145e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-29258cc5-97e9-4570-b435-e95ab243d664\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-29258cc5-97e9-4570-b435-e95ab243d664\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"simonececchinato\",\"key\":\"aa3d76c71079684e73aaa4e257a6898b\"}'}"]},"metadata":{},"execution_count":2}],"source":["files.upload()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1675950425148,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"tgTBTr6f5yv8","outputId":"55d6822c-d9e6-404b-c115-cd3844915483"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","ref                                                             title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","--------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","ahsan81/hotel-reservations-classification-dataset               Hotel Reservations Dataset                         480KB  2023-01-04 12:50:31           9939        333  1.0              \n","googleai/musiccaps                                              MusicCaps                                          793KB  2023-01-25 09:25:48           2129        201  0.9411765        \n","themrityunjaypathak/most-subscribed-1000-youtube-channels       Most Subscribed 1000 Youtube Channels               28KB  2023-01-21 14:42:05           2991         89  1.0              \n","nitishsharma01/olympics-124-years-datasettill-2020              Olympics 124 years Dataset(till 2020)                5MB  2023-02-01 09:00:49           1087         32  1.0              \n","thedevastator/medical-student-mental-health                     Medical Student Mental Health                       19KB  2023-01-25 01:00:14           1208         42  1.0              \n","senapatirajesh/netflix-tv-shows-and-movies                      Latest Netflix TV shows and movies                   1MB  2023-01-14 17:03:12           3741        102  0.9411765        \n","thedevastator/predicting-credit-card-customer-attrition-with-m  Predicting Credit Card Customer Segmentation       379KB  2023-01-15 15:27:38           1385         46  1.0              \n","kane6543/most-watched-stocks-of-past-decade20132023             Most Watched Stocks of Past Decade(2013-2023)        2MB  2023-01-30 15:12:25            918         35  1.0              \n","karkavelrajaj/amazon-sales-dataset                              Amazon Sales Dataset                                 2MB  2023-01-17 06:21:15           3111         78  1.0              \n","thedevastator/us-film-industry-top-movies-directors             US Film Industry Top Movies & Directors            217KB  2023-01-22 08:57:44            677         29  1.0              \n","salimwid/latest-top-3000-companies-ceo-salary-202223            CEO vs Worker Pay in Top 3000 US Companies [2023]  183KB  2023-02-02 09:44:56            443         29  1.0              \n","tymekurban/new-cars-usa-202223-dataset                          New Cars USA 2022/23 dataset                       110KB  2023-01-19 12:09:19            808         30  1.0              \n","abhishek14398/salary-dataset-simple-linear-regression           Salary Dataset - Simple linear regression           457B  2023-01-10 03:55:40           2342         58  1.0              \n","salimwid/technology-company-layoffs-20222023-data               Technology Company Layoffs (2022-2023)              13KB  2023-01-24 07:07:51           1597         43  1.0              \n","thedevastator/higher-education-predictors-of-student-retention  Predict students' dropout and academic success      87KB  2023-01-03 09:18:54           3287         87  1.0              \n","rhugvedbhojane/fifa-world-cup-2022-players-statistics           FIFA World Cup 2022 Players Statistics              98KB  2023-01-27 20:27:08           1290         37  0.8235294        \n","karimabdulnabi/car-price                                        Car Price                                           50KB  2023-01-28 23:09:39            829         25  1.0              \n","rishikeshkonapure/home-loan-approval                            Home Loan Approval                                  13KB  2023-01-12 06:28:57           2906         61  1.0              \n","rakkesharv/spotify-top-10000-streamed-songs                     Spotify Top 10000 Streamed Songs                   280KB  2023-01-02 08:17:15           3472         96  1.0              \n","thedevastator/canine-intelligence-and-size                      Dogs Intelligence and Size                           4KB  2023-01-21 22:09:31           1214         53  0.9411765        \n"]}],"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! kaggle datasets list"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51080,"status":"ok","timestamp":1675950476222,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"4X-z45AP54t6","outputId":"8720ed0f-0946-4d9d-e639-deacb6ea4468"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading animal-image-classification-dataset.zip to /content\n","100% 1.47G/1.47G [00:50<00:00, 38.8MB/s]\n","100% 1.47G/1.47G [00:50<00:00, 31.5MB/s]\n"]}],"source":["!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"]},{"cell_type":"markdown","metadata":{"id":"Ep7psYmO6Dmv"},"source":["The data have been downloaded. To unzip them"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ldwePf3c59cJ","executionInfo":{"status":"ok","timestamp":1675950491333,"user_tz":-60,"elapsed":15133,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["!mkdir /content/animal_data\n","!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"]},{"cell_type":"markdown","metadata":{"id":"5sILCvtJ8rbC"},"source":["## 1.2 COCO dataset"]},{"cell_type":"markdown","metadata":{"id":"grBxE-GZ8v1g"},"source":["To download it we use fastai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Of1FTMIW8VXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675778202767,"user_tz":-60,"elapsed":76681,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"8effd493-cfd8-412a-99fd-5da16136de67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fastai==2.4\n","  Downloading fastai-2.4-py3-none-any.whl (187 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.9/187.9 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastcore<1.4,>=1.3.8\n","  Downloading fastcore-1.3.29-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<1.10,>=1.7.0\n","  Downloading torch-1.9.1-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (3.4.4)\n","Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (3.2.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (6.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.7.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (2.25.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.3.5)\n","Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (1.0.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (23.0)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (0.14.1+cu116)\n","Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastai==2.4) (22.0.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.0.12)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.10.1)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.7.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.4.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.3.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (3.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (0.10.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.21.6)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (6.3.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (8.1.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.11.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (4.64.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (2.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.10.4)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.4) (1.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.4) (4.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.10,>=1.7.0->fastai==2.4) (4.4.0)\n","Collecting torchvision>=0.8.2\n","  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.10.1-cp38-cp38-manylinux1_x86_64.whl (22.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.4) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai==2.4) (2022.7.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.4) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.4) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==2.4) (1.15.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.4) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.4) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai==2.4) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4->fastai==2.4) (2.0.1)\n","Installing collected packages: torch, fastcore, torchvision, fastai\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: fastcore\n","    Found existing installation: fastcore 1.5.28\n","    Uninstalling fastcore-1.5.28:\n","      Successfully uninstalled fastcore-1.5.28\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","  Attempting uninstall: fastai\n","    Found existing installation: fastai 2.7.10\n","    Uninstalling fastai-2.7.10:\n","      Successfully uninstalled fastai-2.7.10\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fastai-2.4 fastcore-1.3.29 torch-1.9.1 torchvision-0.10.1\n"]}],"source":["!pip install fastai==2.4"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"PGmlvToL83YY","executionInfo":{"status":"ok","timestamp":1675950494568,"user_tz":-60,"elapsed":3256,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["from fastai.data.external import untar_data, URLs\n","import os\n","import glob\n","import numpy as np"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":353981,"status":"ok","timestamp":1675950848538,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"0oQGGmVW83uq","outputId":"044fe1cf-1e26-4742-ab8f-6421dc379a1f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='3245883392' class='' max='3245877008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [3245883392/3245877008 05:29&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["# coco images: 21837\n"]}],"source":["coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","\n","paths = glob.glob(coco_path+\"/*.jpg\")\n","paths =np.array(paths)\n","num_images_coco = len(paths)\n","print(f\"# coco images: {num_images_coco}\")"]},{"cell_type":"markdown","metadata":{"id":"PDax1udw8PRC"},"source":["Choose one of the dataset among the two text files: \"data_big_training.txt\" (16k images of which 4.2k animal images) or \"data_small_training.txt\" (9.6k images of which 3k animal images).\n","Since we want to test the generators, choose the big one."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":23808,"status":"ok","timestamp":1675951059525,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"WhULxpYT8Sla","outputId":"3e080bc1-fef3-481f-c4f0-28f0006fc9ae"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6b336fe2-d94e-44b2-aa06-154bde34846b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6b336fe2-d94e-44b2-aa06-154bde34846b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving data_big_training.txt to data_big_training.txt\n"]}],"source":["files.upload();"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6D-j-Gmw8jpP","executionInfo":{"status":"ok","timestamp":1675951059525,"user_tz":-60,"elapsed":8,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["filename = \"data_big_training.txt\" #choose the proper file name\n","\n","def read_lines(path):\n","\n","  lines = None\n","\n","  with open(path) as file:\n","    lines = [line.rstrip() for line in file]\n","\n","  return lines"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675951059525,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"C4kkkSVU8sUl","outputId":"2be5bb36-6d45-42d2-b4d0-f0fc1794cbba"},"outputs":[{"output_type":"stream","name":"stdout","text":["16000 images for training\n"]}],"source":["training_paths = read_lines(filename)\n","print(f\"{len(training_paths)} images for training\")"]},{"cell_type":"markdown","source":["Upload the test dataset \"test_animals.txt\"\n"],"metadata":{"id":"BNNigwADqIjd"}},{"cell_type":"code","source":["files.upload();"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"IsvBcEl7qGa8","executionInfo":{"status":"ok","timestamp":1675951070146,"user_tz":-60,"elapsed":10627,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"f6b39145-363a-42ce-ba9b-92867fda9a82"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c2fdbfb4-472e-4ade-808b-03938cd60e9c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c2fdbfb4-472e-4ade-808b-03938cd60e9c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving test_animals.txt to test_animals.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"7cm997whNLuP"},"source":["#2 Loading of all the generators"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"pfCB0Z44RDzz","executionInfo":{"status":"ok","timestamp":1675951070669,"user_tz":-60,"elapsed":526,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"J922PrOlTwls","executionInfo":{"status":"ok","timestamp":1675951070670,"user_tz":-60,"elapsed":4,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["class UNetDown(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4, normalization_type = None, dropout = 0.0, activation = None):\n","\n","    super(UNetDown, self).__init__()\n","\n","    #if batchnorm/instancenorm used, bias not used\n","\n","    use_bias = normalization_type == None\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","        \n","    if activation == None:\n","      layers.append(nn.LeakyReLU(negative_slope = 0.2))\n","\n","    if activation == \"ReLU\":\n","\n","      layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x):\n","\n","    return self.model(x)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"deGc1E9ST2_8","executionInfo":{"status":"ok","timestamp":1675951071703,"user_tz":-60,"elapsed":3,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["class UNetUp(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4,  normalization_type = None, dropout = 0.0):\n","\n","    super(UNetUp, self).__init__()\n","\n","    use_bias = normalization_type == None\n","\n","    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","\n","    layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x, skip = None):\n","      x = self.model(x)\n","      if skip is not None:\n","\n","        x = torch.cat((skip, x), 1)\n","\n","      return x"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"B34EsSXxT_Nr","executionInfo":{"status":"ok","timestamp":1675951073359,"user_tz":-60,"elapsed":2,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["class GeneratorUNet(nn.Module):\n","\n","  def __init__(self, in_channels = 1, out_channels = 2, num_down = 8, ngf = 64, normalization_type = None):\n","\n","    super(GeneratorUNet, self).__init__()\n","\n","    self.downs = nn.ModuleList()\n","    self.ups = nn.ModuleList()\n","    \n","\n","    features =[ngf]\n","\n","    for i in range(3):\n","\n","      features.append(features[i]*2)\n","\n","    features.append(features[-1])\n","    #64, 128, 256, 512, 512\n","\n","    if num_down > 5:\n","\n","      features += [ngf * 8 for i in range(num_down - 5)]\n","    #for num_down = 8: 64, 128, 256, 512, 512, 512, 512, 512 (->1x1 for input size 256x256)\n","\n","\n","    #ENCODER (CONTRACTING PATH)\n","\n","    #outermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, ngf, 4))\n","\n","    in_channels = ngf #new in_channels for the next down-block\n","    \n","    for i,n_features in enumerate(features[1:len(features)-1]):\n","      #no dropout\n","      self.downs.append(UNetDown(in_channels, n_features, 4, normalization_type, 0.0))\n","      in_channels = n_features\n","\n","    \n","    #innermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, features[-1], 4, activation = \"ReLU\"))\n","    \n","\n","    #DECODER (EXPANSIVE PATH)\n","    i_channels = in_channels\n","    for i, n_features in enumerate((features[-2::-1])):\n","      \n","      \n","      #if i == 0, innermost(bottleneck), namely a block such that after down we go up. no dropout\n","      i_channels = in_channels if i == 0  else i_channels * 2\n","\n","      #no dropout for the first up and the last 4 ups \n","      dropout = 0.0 if (i == 0 or i  > 3) else 0.5\n","\n","      self.ups.append(UNetUp(i_channels, n_features, 4, normalization_type, dropout))\n","      i_channels = n_features\n","    \n","    \n","    self.final = nn.Sequential(\n","        nn.ConvTranspose2d(ngf*2,out_channels, kernel_size=4, stride=2, padding=1),\n","        nn.Tanh()\n","    )\n","\n","\n","\n","  def forward(self, x):\n","\n","    skip_connections = list()\n","\n","    #encoder\n","    for down in self.downs:\n","\n","      x = down(x)\n","      skip_connections.append(x)\n","\n","    #decoder with skip connections\n","    for i, up in enumerate(self.ups):\n","      \n","      x = up(x, skip_connections[-i-2])\n","\n","    return self.final(x)"]},{"cell_type":"markdown","source":["## 2.1 Connect to drive and load the .pt for the generators"],"metadata":{"id":"uqh3KsaCcnBc"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"i15rRWLzPPQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675951103498,"user_tz":-60,"elapsed":20831,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"5ec519d5-8fd4-4081-8f22-1dc234dd6609"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Og6dkepnPVNf","executionInfo":{"status":"ok","timestamp":1675951109912,"user_tz":-60,"elapsed":4516,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["!cp -r /content/drive/MyDrive/TrainedNets/WGAN_9k_120.pt /content/ #WGAN generator"]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_big.pt /content/ # cGAN generator with 16k dataset"],"metadata":{"id":"sNWCGTAUlunn","executionInfo":{"status":"ok","timestamp":1675952070142,"user_tz":-60,"elapsed":4275,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_16_100.pt /content/ # cGAN generator with 9.6k dataset"],"metadata":{"id":"Sdw94A0qn6d7","executionInfo":{"status":"ok","timestamp":1675952081157,"user_tz":-60,"elapsed":5096,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_32_100.pt /content/"],"metadata":{"id":"xVTzi8c4q-Eq","executionInfo":{"status":"ok","timestamp":1675952085807,"user_tz":-60,"elapsed":4655,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_8.pt /content/"],"metadata":{"id":"RkVu1fdvMW1E","executionInfo":{"status":"ok","timestamp":1675952088758,"user_tz":-60,"elapsed":2972,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","execution_count":47,"metadata":{"id":"bIh__oBuNSJ7","executionInfo":{"status":"ok","timestamp":1675952088759,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["# Add the .pt you want to test\n","G_paths = [\"WGAN_9k_120.pt\", \"cGAN_big.pt\", \"cGAN_small_16_100.pt\", \"cGAN_small_32_100.pt\", \"cGAN_small_8.pt\" ]"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"Batve4YsQLBR","executionInfo":{"status":"ok","timestamp":1675952088760,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["def load_generator(G, path = \"/content/cGAN-gen.pt\"):\n","  G.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"-ycoYUXeRyLR","executionInfo":{"status":"ok","timestamp":1675952093665,"user_tz":-60,"elapsed":4108,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["Gs = [] # Array in which we store all the generators\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","for i, G_path in enumerate(G_paths):\n","\n","  Gs.append( GeneratorUNet(1,2,8,64, \"batchnorm\").to(device) )\n","  Gs[i].eval()\n","\n","  load_generator(Gs[i], \"/content/\" + G_path)"]},{"cell_type":"markdown","metadata":{"id":"JKYMOUa_G26b"},"source":["# 3 Visualize Results: show results with some test images"]},{"cell_type":"markdown","source":["With the following cells you can plot the results with some test images and save them in a jpeg image."],"metadata":{"id":"OckBxFv7pkTT"}},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675951128066,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"7noCSFOtHY-Q","outputId":"77ea720c-4d0e-4461-e418-2da8d189ef70"},"outputs":[{"output_type":"stream","name":"stdout","text":["2400 animal images for testing\n"]}],"source":["test_animals_paths = read_lines(\"test_animals.txt\")\n","print(f\"{len(test_animals_paths)} animal images for testing\")"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3872,"status":"ok","timestamp":1675951131933,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"},"user_tz":-60},"id":"nT_h3up8HvIM","outputId":"76ca898a-dfa1-4637-a16d-84543f6fd9ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["10037 coco images for testing\n"]}],"source":["test_coco_paths = []\n","\n","for path in paths:\n","  \n","  if path not in training_paths:\n","    test_coco_paths.append(path)\n","\n","print(f\"{len(test_coco_paths)} coco images for testing\")\n"]},{"cell_type":"markdown","metadata":{"id":"hRll8rzmI1j_"},"source":["## 3.1 Dataset and Dataloader"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"AfVGq77MHvPv","executionInfo":{"status":"ok","timestamp":1675951131934,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["SIZE = 256\n","\n","test_transform = transforms.Compose([\n","                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n","                #transforms.RandomHorizontalFlip(),\n","            ])"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"uSRZsCzCJDkL","executionInfo":{"status":"ok","timestamp":1675951131934,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["class GrayToColorDataset(Dataset):\n","\n","  def __init__(self, paths, transform = None):\n","    \n","    self.paths = paths\n","    self.transform = transform\n","\n","  def __len__(self):\n","\n","    return len(self.paths)\n","\n","  def __getitem__(self, idx):\n","\n","    img_rgb = Image.open(self.paths[idx]).convert(\"RGB\")\n","    img_rgb = self.transform(img_rgb)\n","    img_rgb = np.array(img_rgb)\n","\n","    #RGB -> Lab\n","    img_lab = rgb2lab(img_rgb).astype(\"float32\")\n","    img_lab = transforms.ToTensor()(img_lab)\n","\n","    #to have values in range [-1,1]\n","    L = img_lab[[0],:]/50. - 1.\n","    ab = img_lab[[1,2],:] / 110.\n","\n","    return (L,ab)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"RBPWKFJMJICR","executionInfo":{"status":"ok","timestamp":1675951131935,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["test_coco_dataset = GrayToColorDataset(test_coco_paths, test_transform)\n","test_animals_dataset = GrayToColorDataset(test_animals_paths, test_transform)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"lkgVd40XJZr4","executionInfo":{"status":"ok","timestamp":1675951131935,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["\n","PIN_MEMORY = True\n","N_WORKERS = 2\n","BATCH_SIZE = 9\n","\n","test_coco_dataloader = DataLoader(test_coco_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n","                            pin_memory=PIN_MEMORY, shuffle = False)\n","\n","test_animals_dataloader = DataLoader(test_animals_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n","                            pin_memory=PIN_MEMORY, shuffle = True)"]},{"cell_type":"markdown","metadata":{"id":"5LOVOSm0b9QU"},"source":["## 3.2 Plot and save results"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"T1KbHvSvLIH5","executionInfo":{"status":"ok","timestamp":1675951131936,"user_tz":-60,"elapsed":8,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"outputs":[],"source":["def convert_lab_to_rgb(L, ab):\n","\n","  \"\"\"\n","  Provided a Lab image or a batch of Lab images, it returns it/them in RGB format \n","  input:\n","    - L: torch.tensor\n","    - ab: torch.tensor\n","  \n","  output:\n","    - img: numpy.ndarray (the rgb images)\n","  \"\"\"\n","\n","  #check shape (one image or a batch)\n","\n","  is_batch = len(ab.shape) > 3\n","  \n","  L = (L+1.)*50.\n","  ab = ab*110.\n","\n","  if is_batch:\n","    # input tensors: N x 1 x 256 x 256, N x 2 x 256 x 256\n","    Lab_images = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().detach().numpy()\n","  else:\n","    # input tensors: 1 x 256 x 256, 2 x 256 x 256\n","    Lab_image = torch.cat([L, ab], dim=0).permute(1, 2, 0).cpu().detach().numpy()\n","    return lab2rgb(Lab_image)\n","\n","  rgb_images = list()\n","\n","  for image in Lab_images:\n","\n","    img_rgb = lab2rgb(image)\n","    rgb_images.append(img_rgb)\n","\n","  return np.stack(rgb_images, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLlZtQmDLANa"},"outputs":[],"source":["def show_results(Ls, real_abs, fake_abs, path):\n","\n","  \"\"\"\n","  provided a batch of real and fake images, visualize them (+ the gray images)\n","  input:\n","    - Ls: batch with L for each image, N x 1 x 256 x 256 tensor\n","    - real_abs: batch with ab for each real image, N x 2 x 256 x 256 tensor\n","    - fake_abs: batch with ab for each fake image, N x 2 x 256 x 256 tensor\n","  \"\"\"\n","\n","  n_cols = Ls.shape[0]\n","\n","  real_images = convert_lab_to_rgb(Ls, real_abs)\n","  fake_images = convert_lab_to_rgb(Ls, fake_abs)\n","\n","  fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n","\n","  for idx in range(3):\n","\n","    axes[0,idx].axis(\"off\")\n","    axes[0,idx].imshow(real_images[ 3*idx], aspect = \"auto\")\n","\n","    axes[1,idx].axis(\"off\")\n","    axes[1,idx].imshow(real_images[ 3*idx + 1], aspect = \"auto\")\n","\n","    axes[2,idx].axis(\"off\")\n","    axes[2,idx].imshow(real_images[ 3*idx + 2], aspect = \"auto\")\n","  plt.subplots_adjust(wspace=0.05, hspace = .05)\n","  plt.savefig(path + \"_real.jpg\")\n","  plt.show()\n","  \n","\n","  fig_fake, axes_fake = plt.subplots(3, 3, figsize=(20, 20))\n","\n","  for idx in range(3):\n","\n","    axes_fake[0,idx].axis(\"off\")\n","    axes_fake[0,idx].imshow(fake_images[ 3*idx ], aspect = \"auto\")\n","\n","    axes_fake[1,idx].axis(\"off\")\n","    axes_fake[1,idx].imshow(fake_images[ 3*idx + 1], aspect = \"auto\")\n","\n","    axes_fake[2,idx].axis(\"off\")\n","    axes_fake[2,idx].imshow(fake_images[ 3*idx + 2], aspect = \"auto\")\n","  plt.subplots_adjust(wspace=0.05, hspace = .05)\n","  plt.savefig(path + \"_fake.jpg\")\n","  plt.show()\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_w-5Nc7mgY38"},"outputs":[],"source":["def show_images_for_model(G, dataloader, path):\n","\n","  Ls, abs = next(iter(dataloader))\n","  Ls = Ls.to(device)\n","  abs = abs.to(device)\n","  abs_fake = G(Ls)\n","  show_results(Ls, abs, abs_fake, path)\n","  \n","\n"]},{"cell_type":"markdown","source":["Choose the model to test: change idx to choose a different model. Have a look at Gs list to select the proper index."],"metadata":{"id":"7N_yD8H9cHxm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdJpSiHeXjfL"},"outputs":[],"source":["import random\n","np.random.seed(123)\n","random.seed(10)\n","\n","show_images_for_model(Gs[5], test_animals_dataloader, \"cGAN_small_mix\")"]},{"cell_type":"markdown","source":["Now you can download the .jpg images"],"metadata":{"id":"0VsWxgoZcBLs"}},{"cell_type":"markdown","metadata":{"id":"j96HijJqr2eE"},"source":["# 5 Evaluate Generator - first metric"]},{"cell_type":"markdown","source":["## 5.1 Load classifiers"],"metadata":{"id":"LVjt3rrieCmR"}},{"cell_type":"markdown","source":["Load on colab the two .pt files for the two classifiers (one for colored images and the other for gray images)"],"metadata":{"id":"2he1dsR_fSzo"}},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/vgg16-color.pt /content/"],"metadata":{"id":"brfwjMuzfPal","executionInfo":{"status":"ok","timestamp":1675951150476,"user_tz":-60,"elapsed":14554,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"id":"ixCb6qElr-2g","executionInfo":{"status":"ok","timestamp":1675951198835,"user_tz":-60,"elapsed":10421,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["7faf8e20bd374646b8e55db4f4d3f58c","d297c3ef931c4e78b48932d732867939","c949bb2272f34c36a16e04d4ad628341","9f66940f1f354fa9aa5eb61852f92f65","b825527069224a2f9b03c602606d330a","d23a7611fa0a481f962393aa26914912","0c048192503e49cd9e1aeffec10190ec","50ad69aa14b54196af25c35d14346aaa","00a7c3b2ba5341cf860e21c57135829f","848ff968d9d04406b7fb185da2003449","5e101312ffdf49d28ea65cf3c738045a"]},"outputId":"9b0d1874-78e6-4c0f-d412-262f644c983e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7faf8e20bd374646b8e55db4f4d3f58c"}},"metadata":{}}],"source":["from torchvision import models, transforms\n","\n","C_c = models.vgg16(pretrained=True); #Classifier for color images\n","\n","C_c.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","\n","C_c.load_state_dict(torch.load(\"/content/vgg16-color.pt\"))\n","\n","C_c = C_c.to(device);\n","\n","\n","C_c.eval();"]},{"cell_type":"markdown","source":["## 5.2 Dataset and Dataloader"],"metadata":{"id":"X5H-m5puhEXc"}},{"cell_type":"code","source":["import os\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd"],"metadata":{"id":"7OIhGBywnEAW","executionInfo":{"status":"ok","timestamp":1675951199619,"user_tz":-60,"elapsed":788,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def build_test_dataset(test_path = \"test_animals.txt\"):\n","\n","  #starting path for the kaggle dataset\n","  start_path = '/content/animal_data/Animal Image Dataset/'\n","\n","  images = []\n","  labels = []\n","\n","  with open(test_path) as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","    for path in val_paths:\n","      label = path.split('/')[4]\n","      images.append(path)\n","      labels.append(label)\n","      \n","  data = {'Images':images, 'Labels':labels} \n","  data = pd.DataFrame(data) \n","\n","  lb = LabelEncoder()\n","  data['encoded_labels'] = lb.fit_transform(data['Labels'])\n","\n","  return data\n","\n"],"metadata":{"id":"IITFKzhymPtu","executionInfo":{"status":"ok","timestamp":1675951199620,"user_tz":-60,"elapsed":9,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["data = build_test_dataset(test_path = \"test_animals.txt\")"],"metadata":{"id":"F5sj1uJuoxM4","executionInfo":{"status":"ok","timestamp":1675951199620,"user_tz":-60,"elapsed":8,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Transform used by the classifiers\n","trans_classifier = transforms.Compose([\n","                  transforms.Resize((224,224)),\n","                  transforms.ToTensor(),\n","                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","                  ])\n","\n","# Transform used to convert the result from the generator to the input format for the classifier\n","trans_gan_to_classifier = transforms.Compose([\n","                  transforms.Resize((224,224)),\n","                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","                  ])\n","\n","# Transform for the GAN\n","trans_gan = transforms.Compose([\n","                transforms.Resize((256, 256),  transforms.InterpolationMode.BILINEAR),\n","            ])"],"metadata":{"id":"UGPYLv4ChKzj","executionInfo":{"status":"ok","timestamp":1675951199620,"user_tz":-60,"elapsed":7,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["The following dataset will allow us to get all the input for the classifiers and for the generator (in particular the color image, the grayscale image and the label)"],"metadata":{"id":"4tibhIn4tKki"}},{"cell_type":"code","source":["class Animals_Dataset(Dataset):\n","    def __init__(self, img_data, tr1, tr2):\n","        self.tr1 = tr1\n","        self.tr2 = tr2\n","        self.img_data = img_data\n","        \n","    def __len__(self):\n","        return len(self.img_data)\n","    \n","    def __getitem__(self, index):\n","        img_name = self.img_data.loc[index, 'Images']\n","\n","        #format for classifiers\n","        image = Image.open(img_name)\n","        image = image.convert('RGB')\n","        gray = image.convert('L')\n","        gray_image = gray.convert('RGB')\n","\n","        #format for cGAN, WGAN\n","        img_rgb = self.tr2(image)\n","        img_np = np.array(img_rgb)\n","        img_lab = rgb2lab(img_np).astype(\"float32\")\n","        img_lab = transforms.ToTensor()(img_lab)\n","\n","        L = img_lab[[0],:] /50.-1.\n","        \n","        \n","        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n","        \n","        if self.tr1 is not None:\n","            image = self.tr1(image)\n","            gray_image = self.tr1(gray_image)\n","        \n","        return image, gray_image, L, label"],"metadata":{"id":"XIYf0FEHhOmO","executionInfo":{"status":"ok","timestamp":1675951199621,"user_tz":-60,"elapsed":8,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["test_dataset = Animals_Dataset(data, trans_classifier, trans_gan)"],"metadata":{"id":"RXCs9sMmhRWt","executionInfo":{"status":"ok","timestamp":1675951199621,"user_tz":-60,"elapsed":8,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n","                                                shuffle = False)"],"metadata":{"id":"GE5Tj-bbo45H","executionInfo":{"status":"ok","timestamp":1675951201068,"user_tz":-60,"elapsed":3,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Function for the evaluation"],"metadata":{"id":"_HbLkXGDpxIH"}},{"cell_type":"code","source":["def evaluate_generator(G):\n","\n","  softmax = nn.Softmax(dim = 1)\n","  G.eval()\n","\n","  weighted_sum_fake = 0.0\n","  to_normalize = 0.0\n","\n","  weighted_sum_real = 0.0\n","  weighted_sum_gray = 0.0\n","\n","  sum_fake = 0.0\n","  sum_real = 0.0\n","  sum_gray = 0.0\n","\n","  accuracy_fake = 0.0\n","  accuracy_color = 0.0\n","  accuracy_gray = 0.0\n","\n","  for batch in tqdm(test_loader):\n","\n","    with torch.no_grad():\n","      image = batch[0].to(device)\n","      gray = batch[1].to(device)\n","      L = batch[2].to(device)\n","      labels = batch[3].to(device)\n","\n","      #FORWARD\n","\n","      prob_color_s = softmax(C_c(image))\n","      prob_gray_c = softmax(C_c(gray))\n","\n","      ab_fake = G(L)\n","\n","      #from GAN output to RGB (input classifier)\n","      rgb_fake = torch.from_numpy(convert_lab_to_rgb(L, ab_fake)).permute(0,3,1,2)\n","      rgb_fake = trans_gan_to_classifier(rgb_fake).to(device)\n","\n","      #output_fake: output C_c with fake images \n","      output_fake = C_c(rgb_fake) # logits\n","      \n","      #labels predicted by C_c with fake images\n","      pred_labels_fake = torch.argmax(output_fake, dim = 1)\n","\n","\n","      #select probs for the correct classes\n","      prob_color = prob_color_s[np.arange(len(prob_color_s)),labels]\n","      prob_fake = softmax(output_fake)[np.arange(len(prob_gray_c)),labels]\n","\n","      prob_gray = prob_gray_c[np.arange(len(prob_gray_c)),labels]\n","\n","      #weights computation\n","      weights = torch.abs(prob_color - prob_gray)\n","\n","      #update unnormalized sum and sum of weights\n","      weighted_sum_fake += torch.sum(weights*prob_fake)\n","      weighted_sum_real += torch.sum(weights*prob_color)\n","      weighted_sum_gray += torch.sum(weights*prob_gray)\n","\n","\n","      to_normalize += torch.sum(weights)\n","\n","      sum_fake += torch.sum(prob_fake)\n","      sum_gray += torch.sum(prob_gray)\n","      sum_real += torch.sum(prob_color)\n","\n","      #ACCURACY\n","      accuracy_fake += torch.sum(pred_labels_fake == labels)\n","      accuracy_color += torch.sum(torch.argmax(prob_color_s, dim= 1) == labels)\n","      accuracy_gray += torch.sum(torch.argmax(prob_gray_c, dim = 1) == labels)\n","\n","\n","  weighted_multinoulli_pred_real = (weighted_sum_real / to_normalize).item()\n","  weighted_multinoulli_pred_gray = (weighted_sum_gray / to_normalize).item()\n","  weighted_multinoulli_pred_fake = (weighted_sum_fake/ to_normalize).item()\n","\n","  multinoulli_pred_fake = (sum_fake / len(test_dataset) ).item()\n","  multinoulli_pred_real = (sum_real / len(test_dataset) ).item()\n","  multinoulli_pred_gray = (sum_gray / len(test_dataset) ).item()\n","\n","  accuracy_fake = ( accuracy_fake / len(test_dataset) ).item()\n","  accuracy_real = ( accuracy_color / len(test_dataset) ).item()\n","  accuracy_gray = ( accuracy_gray / len(test_dataset) ).item()\n","\n","  print(f\"Weighting metric, fake: {weighted_multinoulli_pred_fake}\")\n","  print(f\"Weighting metric, real: {weighted_multinoulli_pred_real}\") \n","  print(f\"Weighting metric, gray: {weighted_multinoulli_pred_gray}\")\n","  \n","  print()\n","\n","  print(f\"Accuracy metric, fake: {accuracy_fake}\")\n","  print(f\"Accuracy metric, real: {accuracy_real}\") \n","  print(f\"Accuracy metric, gray: {accuracy_gray}\") "],"metadata":{"id":"gdyctAb7p2aB","executionInfo":{"status":"ok","timestamp":1675951547174,"user_tz":-60,"elapsed":2,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import warnings\n","#[\"WGAN_9k_120.pt\", \"cGAN_big.pt\", \"cGAN_small_16_100.pt\", \"cGAN_small_32_100.pt\", \"cGAN_small_8.pt\" ]"],"metadata":{"id":"k0zIuM42yedf","executionInfo":{"status":"ok","timestamp":1675951233672,"user_tz":-60,"elapsed":543,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174,"referenced_widgets":["5e097f2a1b2d49de833c8f1e1d5c0778","f954247e96bc4214b8fc1e32a915fcf8","7c26cf094a094189bf59580c8eca4547","20f0ba171d8849848c9eb58321eb1ced","0ac768eb595d4b27a986a220032dab20","ae1c084641a547589a82d22475d4af8b","d766a930771d4c779dbffce263eaa0bc","8f4baf698b324419a112a316c02ed81a","5946236e3b124dfda38798231219e666","d57cf921fe9f498191a30b66968a8134","b7f7c8d363294dc2b8145bcba29a0f31"]},"id":"XXBbjQulm3rU","executionInfo":{"status":"ok","timestamp":1675951725789,"user_tz":-60,"elapsed":177106,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"01ab2727-aebf-4743-c1f1-1903ad26633e"},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/150 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e097f2a1b2d49de833c8f1e1d5c0778"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Weighting metric, fake: 0.5358761548995972\n","Weighting metric, real: 0.8370403051376343\n","Weighting metric, gray: 0.4612634778022766\n","\n","Accuracy metric, fake: 0.9145833849906921\n","Accuracy metric, real: 0.9658333659172058\n","Accuracy metric, gray: 0.9362500309944153\n"]}]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174,"referenced_widgets":["fb49b456b12545edae985249cd1d05f6","7f425e22649e47a592224ffdcbda5861","a168d7b21c79427e844b99f465f13451","ac1792f90d92489986c148b3549d1984","bc975e434fe64d549abda756853a1ade","a097577fc0454e4e9492f80f6accfd1e","18dc6fedb5ab46d1963d96ed416fd30e","1191424978ca4210b9481b731e3b8740","ca385decf3b74ceeb5487f82db3e3fc8","766e5f23fe3d474db221490eaed4d9ef","65d829a65e68493693c5d62d109b5885"]},"id":"Jb06A55jlexy","executionInfo":{"status":"ok","timestamp":1675952273987,"user_tz":-60,"elapsed":179927,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"76d1211d-0dbf-4858-9449-9d9ed1bd0a21"},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/150 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb49b456b12545edae985249cd1d05f6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Weighting metric, fake: 0.5613495111465454\n","Weighting metric, real: 0.8370403051376343\n","Weighting metric, gray: 0.4612634778022766\n","\n","Accuracy metric, fake: 0.9079166650772095\n","Accuracy metric, real: 0.9658333659172058\n","Accuracy metric, gray: 0.9362500309944153\n"]}]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385,"referenced_widgets":["4fe9cfd45448437d80458a668ae894d2","cc8e03e784484b26b59e306d9503e54a","e8adb537c6b345bdaa90fc0ec9d9cd29","ad1c1859856847ba8d6d0357f35a9918","4606a2f609d342598dd7be444233ac4e","a5e8631f896a47a6a97f8e80c41241b0","c83512f87c6840e8902204ae57c49def","566d2141d88d43c889df10dfca93af79","8322e36321374a4d8d4c0e2b2ac3bdbd","53a5a5f2c60c49eca19e0f5536df4ec2","ac324c1a934745e79d6de0fa166268d3"]},"id":"Mm_erF_bokpS","executionInfo":{"status":"error","timestamp":1675952301869,"user_tz":-60,"elapsed":27904,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"a92c5a10-7b6a-4c2d-bfa2-d3751a0532aa"},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/150 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe9cfd45448437d80458a668ae894d2"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-3edc836582d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-41-da3807f4ae94>\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;31m#from GAN output to RGB (input classifier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mrgb_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_lab_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mrgb_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_gan_to_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-6480f3151e77>\u001b[0m in \u001b[0;36mconvert_lab_to_rgb\u001b[0;34m(L, ab)\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# input tensors: N x 1 x 256 x 256, N x 2 x 256 x 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mLab_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# input tensors: 1 x 256 x 256, 2 x 256 x 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[3])"],"metadata":{"id":"GTI1uIPXOt5Z","executionInfo":{"status":"aborted","timestamp":1675952301870,"user_tz":-60,"elapsed":5,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[4])"],"metadata":{"id":"qea7BoRdOuEk","executionInfo":{"status":"aborted","timestamp":1675952301871,"user_tz":-60,"elapsed":6,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Weight comparison - second metric"],"metadata":{"id":"xu9gJzs-GHWL"}},{"cell_type":"markdown","source":["## 6.1 Dataset and dataloader"],"metadata":{"id":"2BKt1ziwNWNf"}},{"cell_type":"code","source":["def build_dataset(path_noadd = \"data_big_training.txt\"):\n","\n","  #starting path for the kaggle dataset\n","  start_path = '/content/animal_data/Animal Image Dataset/'\n","\n","  with open(path_noadd) as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","  images = []\n","  labels = []\n","\n","  for folders, subfolders, files in os.walk(start_path,topdown=True):\n","    label = folders.split('/')[4]\n","    for file in files:\n","\n","      path_file = start_path + label + '/' + file\n","\n","      if path_file not in val_paths:  \n","        images.append(path_file)\n","        labels.append(label)\n","      \n","  data = {'Images':images, 'Labels':labels} \n","  data = pd.DataFrame(data) \n","\n","  lb = LabelEncoder()\n","  data['encoded_labels'] = lb.fit_transform(data['Labels'])\n","\n","  return data"],"metadata":{"id":"bvnVfejhGA22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_data = build_dataset(path_noadd = \"data_big_training.txt\")\n","print(len(new_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2fYB-80GA23","executionInfo":{"status":"ok","timestamp":1675877204809,"user_tz":-60,"elapsed":2422,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"891bd883-cac3-4328-8a31-50da55de5af6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12978\n"]}]},{"cell_type":"code","source":["classifier_dataset = Animals_Dataset(new_data,trans_classifier, trans_gan)"],"metadata":{"id":"eQH8BCl0GA23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_loader = torch.utils.data.DataLoader(classifier_dataset, batch_size=128,\n","                                                shuffle = True)"],"metadata":{"id":"kodP5m7dGA23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.2 Classifier training"],"metadata":{"id":"2V2pxupeQC2E"}},{"cell_type":"markdown","source":["In this second metric we train two different networks. One with the original colored images and one with the images colored by the GAN. If the colorization is good then the weights of the two trained models shouldn't be very different."],"metadata":{"id":"y7B48FstQUqo"}},{"cell_type":"markdown","source":["## 6.2.1 Train first model"],"metadata":{"id":"oJJUDhU9Qs1f"}},{"cell_type":"markdown","source":["Train this model with the original colored images"],"metadata":{"id":"XwNY_5ETQ68m"}},{"cell_type":"code","source":["from torchvision import models, transforms\n","\n","classifier_model = models.vgg16(pretrained=True)\n","classifier_model.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","classifier_model = classifier_model.to(device)"],"metadata":{"id":"C_C9QsrWGA23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(classifier_model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(classifier_loader)"],"metadata":{"id":"RqEtR4-UGA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs = 2\n","print_every = 25\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc = []\n","train_loss = []\n","train_acc = []\n","total_step = len(classifier_loader)\n","\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","    # scheduler.step(epoch)\n","    correct = 0\n","    total=0\n","    print(f'Epoch {epoch}\\n')\n","\n","    for i, (images, _, _, labels) in tqdm(enumerate(classifier_loader), total = len(classifier_loader)):\n","\n","        # Move tensors to the configured device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = classifier_model(images)\n","        #print(outputs)\n","        #print(labels)\n","        loss = criterion(outputs, labels)\n","        #print(loss)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        _,pred = torch.max(outputs, dim=1)\n","        correct += torch.sum(pred==labels).item()\n","        total += labels.size(0)\n","\n","        if (i) % print_every == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, i, total_step, loss.item()))\n","            \n","    train_acc.append(100 * correct / total)\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","    classifier_model.train()"],"metadata":{"id":"tGEG8nFdGA24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save the trained network"],"metadata":{"id":"7nYozDs9Qxo2"}},{"cell_type":"code","source":["torch.save(classifier_model.state_dict(), 'vgg16_real_images.pt')"],"metadata":{"id":"OvJX7OhHGA24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.2.2 Train second model"],"metadata":{"id":"BayxxPzQGA24"}},{"cell_type":"markdown","source":["Train this model with the images colored by the GAN"],"metadata":{"id":"oRmROxE-Q-3N"}},{"cell_type":"code","source":["from torchvision import models, transforms\n","\n","classifier_fake_images = models.vgg16(pretrained=True)\n","classifier_fake_images.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","classifier_fake_images = classifier_fake_images.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91Sb5qbvGA24","executionInfo":{"status":"ok","timestamp":1675877211661,"user_tz":-60,"elapsed":2378,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"346cc690-c960-436d-ee03-e924e6d60455"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer_fake = torch.optim.SGD(classifier_fake_images.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(classifier_loader)"],"metadata":{"id":"PLef5ApqGA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_classifier_generator(G, epochs = 2, print_every = 25):\n","\n","  n_epochs = 2\n","  print_every = 25\n","  valid_loss_min = np.Inf\n","  val_loss = []\n","  val_acc = []\n","  train_loss = []\n","  train_acc = []\n","  total_step = len(classifier_loader)\n","\n","  G.train()\n","  classifier_fake_images.train()\n","\n","  for epoch in range(1, n_epochs+1):\n","      running_loss = 0.0\n","      # scheduler.step(epoch)\n","      correct = 0\n","      total=0\n","      print(f'Epoch {epoch}\\n')\n","\n","      for i, (images, _, L, labels) in tqdm(enumerate(classifier_loader), total = len(classifier_loader)):\n","\n","          # Move tensors to the configured device\n","          images = images.to(device)\n","          labels = labels.to(device)\n","          L = L.to(device)\n","          ab = G(L).detach()\n","\n","          rgb_fake = torch.from_numpy(convert_lab_to_rgb(L, ab)).permute(0,3,1,2)\n","          rgb_fake = trans_gan_to_classifier(rgb_fake).to(device).detach()\n","\n","          optimizer_fake.zero_grad()\n","\n","          outputs = classifier_fake_images(rgb_fake)\n","          loss = criterion(outputs, labels)\n","\n","\n","          loss.backward()\n","          optimizer_fake.step()\n","          \n","          # print statistics\n","          running_loss += loss.item()\n","          _,pred = torch.max(outputs, dim=1)\n","          correct += torch.sum(pred==labels).item()\n","          total += labels.size(0)\n","\n","          if (i) % print_every == 0:\n","              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                    .format(epoch, n_epochs, i, total_step, loss.item()))\n","              \n","      train_acc.append(100 * correct / total)\n","      train_loss.append(running_loss/total_step)\n","      print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","      classifier_fake_images.train()\n"],"metadata":{"id":"iiMUYrr-v8HN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    train_classifier_generator(Gs[4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402,"referenced_widgets":["3c2b7e9f5e454d1bb35db9c4c82f2016","19c63a7253ef45b981e48346b7aab8be","ab3684f19f7d402593cf991b31ea31b7","7e099c0a6fdb4ac9ba3894e11fb29c30","c07541407a4c41dfab16923557292072","bd933cdb750945c49d548cb28dd95289","66b9b6cf7f964e16bf0226a0592a0419","68c305f4cceb49c2b8271bee5a2d5bd3","bb340d9f116a440a8278139a8148dc4e","8fdb9a1ec5544bf88a4264d41aff8a4b","4af06e00bbe746dc9ae386f4aa2a14ad","b9c68896866d4c5ea5bbcdc33842666c","5f3626b24baa420bbb913b58349e7c2a","48f35e557c714fd3adecbf5c5b3da2d7","90008da0ed094cea94d507bb1fc7f320","da51e87912ac467f8bcaef894c332408","3c314d6d097c4271aacd17856d7a3946","dfaf6a14f0b54a30bfd2fead20cb1329","057c30c5b2b24733a5cbb543ebd61a6b","c30cd9d35231499a8c582495c18c5761","bf30a666d1d94dad9ba562ba28757950","8b571f2a9b2e4185bec68dc1c0a01823"]},"id":"_X6L9MF_0oi1","executionInfo":{"status":"ok","timestamp":1675879184540,"user_tz":-60,"elapsed":1968851,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"7c0cd5a9-a4bc-4556-e4dc-2954d3e802a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/102 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2b7e9f5e454d1bb35db9c4c82f2016"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Step [0/102], Loss: 2.5513\n","Epoch [1/2], Step [25/102], Loss: 0.4661\n","Epoch [1/2], Step [50/102], Loss: 0.2305\n","Epoch [1/2], Step [75/102], Loss: 0.2993\n","Epoch [1/2], Step [100/102], Loss: 0.1456\n","\n","train loss: 0.4191, train acc: 86.3076\n","Epoch 2\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/102 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c68896866d4c5ea5bbcdc33842666c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch [2/2], Step [0/102], Loss: 0.1266\n","Epoch [2/2], Step [25/102], Loss: 0.0652\n","Epoch [2/2], Step [50/102], Loss: 0.1558\n","Epoch [2/2], Step [75/102], Loss: 0.1267\n","Epoch [2/2], Step [100/102], Loss: 0.1032\n","\n","train loss: 0.2764, train acc: 95.8468\n"]}]},{"cell_type":"code","source":["torch.save(classifier_fake_images.state_dict(), 'classifier_cGAN_small_8.pt')"],"metadata":{"id":"EGfRc3cq-5hl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To save the classifier"],"metadata":{"id":"GAEVK0uEG2Ln"}},{"cell_type":"code","source":["!cp /content/classifier_cGAN_small_8.pt /content/drive/MyDrive/TrainedNets/Weight_Comparison"],"metadata":{"id":"kvoFiufqG1LH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.3 Weights comparison"],"metadata":{"id":"goV7Fo1CNdCX"}},{"cell_type":"markdown","source":["Apply the L2-norm to the weights of the two models. If this value is small then the recolorization process is good."],"metadata":{"id":"56D9woGyRRJ1"}},{"cell_type":"code","source":["def weights_distance(m_1, m_2) :\n","\n","  m_1_list = list()\n","  for name, p in m_1.named_parameters():\n","    m_1_list.append(p)\n","\n","  m_2_list = list()\n","  for name, p in m_2.named_parameters():\n","    m_2_list.append(p)\n","\n","\n","  diff = list()\n","\n","  for i in range(len(m_1_list)):\n","    diff.append(torch.sum(torch.sub(m_1_list[i], m_2_list[i])**2))\n","\n","\n","  distance = 0.0\n","\n","  for i in range(len(diff)) :\n","    distance += diff[i]\n","\n","  return torch.sqrt(distance).item()"],"metadata":{"id":"b0gEkH45GA25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The smaller the distance, the better it is."],"metadata":{"id":"TdRpFSKINkVC"}},{"cell_type":"code","source":["distance = weights_distance(classifier_model, classifier_fake_images)\n","\n","print(distance) #l2-norm between the weights of the two models"],"metadata":{"id":"q6yDsqsrNiDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.4 Importing saved model to evaluate weight distance"],"metadata":{"id":"E0W19w5r6WPD"}},{"cell_type":"code","source":["!cp /content/drive/MyDrive/TrainedNets/Weight_Comparison/classifier_cGAN_big.pt /content/"],"metadata":{"id":"6wDh0QbuGvYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m_2 = models.vgg16(pretrained=True)\n","m_2.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","m_2.load_state_dict(torch.load(\"/content/vgg16_real_images.pt\"))\n","\n","m_2 = m_2.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jw2mGDts6U72","executionInfo":{"status":"ok","timestamp":1675879386390,"user_tz":-60,"elapsed":2368,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"5e1db89b-0dde-45fd-d18c-fd4be00ea565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["m_1 = models.vgg16(pretrained=True)\n","m_1.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","m_1.load_state_dict(torch.load(\"/content/classifier_cGAN_small_32.pt\"))\n","\n","m_1 = m_1.to(device)"],"metadata":{"id":"VENZkGVNOqxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["distance = weights_distance(m_1, m_2)\n","\n","print(distance) #l2-norm between the weights of the two models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QcYws38bEaQh","executionInfo":{"status":"ok","timestamp":1675879891362,"user_tz":-60,"elapsed":5,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"c6a05e06-73a6-4f49-fb2f-d68110951b74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.416975975036621\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rfMWhK6X_Pwos5oAaeKw9xH-b3FCDb14","timestamp":1675173022259}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3c2b7e9f5e454d1bb35db9c4c82f2016":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19c63a7253ef45b981e48346b7aab8be","IPY_MODEL_ab3684f19f7d402593cf991b31ea31b7","IPY_MODEL_7e099c0a6fdb4ac9ba3894e11fb29c30"],"layout":"IPY_MODEL_c07541407a4c41dfab16923557292072"}},"19c63a7253ef45b981e48346b7aab8be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd933cdb750945c49d548cb28dd95289","placeholder":"​","style":"IPY_MODEL_66b9b6cf7f964e16bf0226a0592a0419","value":"100%"}},"ab3684f19f7d402593cf991b31ea31b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c305f4cceb49c2b8271bee5a2d5bd3","max":102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb340d9f116a440a8278139a8148dc4e","value":102}},"7e099c0a6fdb4ac9ba3894e11fb29c30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fdb9a1ec5544bf88a4264d41aff8a4b","placeholder":"​","style":"IPY_MODEL_4af06e00bbe746dc9ae386f4aa2a14ad","value":" 102/102 [16:26&lt;00:00,  7.66s/it]"}},"c07541407a4c41dfab16923557292072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd933cdb750945c49d548cb28dd95289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66b9b6cf7f964e16bf0226a0592a0419":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68c305f4cceb49c2b8271bee5a2d5bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb340d9f116a440a8278139a8148dc4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fdb9a1ec5544bf88a4264d41aff8a4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af06e00bbe746dc9ae386f4aa2a14ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9c68896866d4c5ea5bbcdc33842666c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f3626b24baa420bbb913b58349e7c2a","IPY_MODEL_48f35e557c714fd3adecbf5c5b3da2d7","IPY_MODEL_90008da0ed094cea94d507bb1fc7f320"],"layout":"IPY_MODEL_da51e87912ac467f8bcaef894c332408"}},"5f3626b24baa420bbb913b58349e7c2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c314d6d097c4271aacd17856d7a3946","placeholder":"​","style":"IPY_MODEL_dfaf6a14f0b54a30bfd2fead20cb1329","value":"100%"}},"48f35e557c714fd3adecbf5c5b3da2d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_057c30c5b2b24733a5cbb543ebd61a6b","max":102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c30cd9d35231499a8c582495c18c5761","value":102}},"90008da0ed094cea94d507bb1fc7f320":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf30a666d1d94dad9ba562ba28757950","placeholder":"​","style":"IPY_MODEL_8b571f2a9b2e4185bec68dc1c0a01823","value":" 102/102 [16:21&lt;00:00,  7.80s/it]"}},"da51e87912ac467f8bcaef894c332408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c314d6d097c4271aacd17856d7a3946":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfaf6a14f0b54a30bfd2fead20cb1329":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"057c30c5b2b24733a5cbb543ebd61a6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c30cd9d35231499a8c582495c18c5761":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf30a666d1d94dad9ba562ba28757950":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b571f2a9b2e4185bec68dc1c0a01823":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7faf8e20bd374646b8e55db4f4d3f58c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d297c3ef931c4e78b48932d732867939","IPY_MODEL_c949bb2272f34c36a16e04d4ad628341","IPY_MODEL_9f66940f1f354fa9aa5eb61852f92f65"],"layout":"IPY_MODEL_b825527069224a2f9b03c602606d330a"}},"d297c3ef931c4e78b48932d732867939":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d23a7611fa0a481f962393aa26914912","placeholder":"​","style":"IPY_MODEL_0c048192503e49cd9e1aeffec10190ec","value":"100%"}},"c949bb2272f34c36a16e04d4ad628341":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50ad69aa14b54196af25c35d14346aaa","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00a7c3b2ba5341cf860e21c57135829f","value":553433881}},"9f66940f1f354fa9aa5eb61852f92f65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_848ff968d9d04406b7fb185da2003449","placeholder":"​","style":"IPY_MODEL_5e101312ffdf49d28ea65cf3c738045a","value":" 528M/528M [00:05&lt;00:00, 118MB/s]"}},"b825527069224a2f9b03c602606d330a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d23a7611fa0a481f962393aa26914912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c048192503e49cd9e1aeffec10190ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50ad69aa14b54196af25c35d14346aaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00a7c3b2ba5341cf860e21c57135829f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"848ff968d9d04406b7fb185da2003449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e101312ffdf49d28ea65cf3c738045a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e097f2a1b2d49de833c8f1e1d5c0778":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f954247e96bc4214b8fc1e32a915fcf8","IPY_MODEL_7c26cf094a094189bf59580c8eca4547","IPY_MODEL_20f0ba171d8849848c9eb58321eb1ced"],"layout":"IPY_MODEL_0ac768eb595d4b27a986a220032dab20"}},"f954247e96bc4214b8fc1e32a915fcf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae1c084641a547589a82d22475d4af8b","placeholder":"​","style":"IPY_MODEL_d766a930771d4c779dbffce263eaa0bc","value":"100%"}},"7c26cf094a094189bf59580c8eca4547":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f4baf698b324419a112a316c02ed81a","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5946236e3b124dfda38798231219e666","value":150}},"20f0ba171d8849848c9eb58321eb1ced":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d57cf921fe9f498191a30b66968a8134","placeholder":"​","style":"IPY_MODEL_b7f7c8d363294dc2b8145bcba29a0f31","value":" 150/150 [02:56&lt;00:00,  1.10it/s]"}},"0ac768eb595d4b27a986a220032dab20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae1c084641a547589a82d22475d4af8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d766a930771d4c779dbffce263eaa0bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f4baf698b324419a112a316c02ed81a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5946236e3b124dfda38798231219e666":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d57cf921fe9f498191a30b66968a8134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7f7c8d363294dc2b8145bcba29a0f31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb49b456b12545edae985249cd1d05f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f425e22649e47a592224ffdcbda5861","IPY_MODEL_a168d7b21c79427e844b99f465f13451","IPY_MODEL_ac1792f90d92489986c148b3549d1984"],"layout":"IPY_MODEL_bc975e434fe64d549abda756853a1ade"}},"7f425e22649e47a592224ffdcbda5861":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a097577fc0454e4e9492f80f6accfd1e","placeholder":"​","style":"IPY_MODEL_18dc6fedb5ab46d1963d96ed416fd30e","value":"100%"}},"a168d7b21c79427e844b99f465f13451":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1191424978ca4210b9481b731e3b8740","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca385decf3b74ceeb5487f82db3e3fc8","value":150}},"ac1792f90d92489986c148b3549d1984":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_766e5f23fe3d474db221490eaed4d9ef","placeholder":"​","style":"IPY_MODEL_65d829a65e68493693c5d62d109b5885","value":" 150/150 [03:00&lt;00:00,  1.08it/s]"}},"bc975e434fe64d549abda756853a1ade":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a097577fc0454e4e9492f80f6accfd1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18dc6fedb5ab46d1963d96ed416fd30e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1191424978ca4210b9481b731e3b8740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca385decf3b74ceeb5487f82db3e3fc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"766e5f23fe3d474db221490eaed4d9ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d829a65e68493693c5d62d109b5885":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fe9cfd45448437d80458a668ae894d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc8e03e784484b26b59e306d9503e54a","IPY_MODEL_e8adb537c6b345bdaa90fc0ec9d9cd29","IPY_MODEL_ad1c1859856847ba8d6d0357f35a9918"],"layout":"IPY_MODEL_4606a2f609d342598dd7be444233ac4e"}},"cc8e03e784484b26b59e306d9503e54a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5e8631f896a47a6a97f8e80c41241b0","placeholder":"​","style":"IPY_MODEL_c83512f87c6840e8902204ae57c49def","value":" 19%"}},"e8adb537c6b345bdaa90fc0ec9d9cd29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_566d2141d88d43c889df10dfca93af79","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8322e36321374a4d8d4c0e2b2ac3bdbd","value":28}},"ad1c1859856847ba8d6d0357f35a9918":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53a5a5f2c60c49eca19e0f5536df4ec2","placeholder":"​","style":"IPY_MODEL_ac324c1a934745e79d6de0fa166268d3","value":" 28/150 [00:27&lt;01:53,  1.08it/s]"}},"4606a2f609d342598dd7be444233ac4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5e8631f896a47a6a97f8e80c41241b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c83512f87c6840e8902204ae57c49def":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"566d2141d88d43c889df10dfca93af79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8322e36321374a4d8d4c0e2b2ac3bdbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53a5a5f2c60c49eca19e0f5536df4ec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac324c1a934745e79d6de0fa166268d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}