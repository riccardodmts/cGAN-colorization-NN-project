{"cells":[{"cell_type":"markdown","metadata":{"id":"e54QtGM-4FGH"},"source":["# 1 Data collection"]},{"cell_type":"markdown","metadata":{"id":"Sk3EcePY4PV1"},"source":["Two datasets are used: a small version of COCO dataset with 21,837 images and one with 17,178 images of animals (12 categories)"]},{"cell_type":"markdown","metadata":{"id":"1B0UnTf94ndc"},"source":["##1.1 Animals dataset"]},{"cell_type":"markdown","metadata":{"id":"QbIXQR5B5Qty"},"source":["We download this dataset from kaggle (1.4 GB)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LyAJsIy4H9R"},"outputs":[],"source":["!pip install -q kaggle\n","from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"gW1I4x0j5dLh"},"source":["You have to upload a file called kaggle.json. To obtain it you need to follow the first 2 steps described in https://www.kaggle.com/general/74235"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcwM9NAY5cJQ"},"outputs":[],"source":["files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgTBTr6f5yv8"},"outputs":[],"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! kaggle datasets list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4X-z45AP54t6"},"outputs":[],"source":["!kaggle datasets download -d piyushkumar18/animal-image-classification-dataset"]},{"cell_type":"markdown","metadata":{"id":"Ep7psYmO6Dmv"},"source":["The data have been downloaded. To unzip them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldwePf3c59cJ"},"outputs":[],"source":["!mkdir /content/animal_data\n","!unzip -qq /content/animal-image-classification-dataset.zip -d /content/animal_data/"]},{"cell_type":"markdown","metadata":{"id":"5sILCvtJ8rbC"},"source":["## 1.2 COCO dataset"]},{"cell_type":"markdown","metadata":{"id":"grBxE-GZ8v1g"},"source":["To download it we use fastai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Of1FTMIW8VXG"},"outputs":[],"source":["!pip install fastai==2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGmlvToL83YY"},"outputs":[],"source":["from fastai.data.external import untar_data, URLs\n","import os\n","import glob\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oQGGmVW83uq"},"outputs":[],"source":["coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","\n","paths = glob.glob(coco_path+\"/*.jpg\")\n","paths =np.array(paths)\n","num_images_coco = len(paths)\n","print(f\"# coco images: {num_images_coco}\")"]},{"cell_type":"markdown","metadata":{"id":"PDax1udw8PRC"},"source":["Choose one of the dataset among the two text files: \"data_big_training.txt\" (16k images of which 4.2k animal images) or \"data_small_training.txt\" (9.6k images of which 3k animal images).\n","Since we want to test the generators, choose the big one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhULxpYT8Sla"},"outputs":[],"source":["files.upload();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6D-j-Gmw8jpP"},"outputs":[],"source":["filename = \"data_big_training.txt\" #choose the proper file name\n","\n","def read_lines(path):\n","\n","  lines = None\n","\n","  with open(path) as file:\n","    lines = [line.rstrip() for line in file]\n","\n","  return lines"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4kkkSVU8sUl"},"outputs":[],"source":["training_paths = read_lines(filename)\n","print(f\"{len(training_paths)} images for training\")"]},{"cell_type":"markdown","source":["Upload the test dataset \"test_animals.txt\"\n"],"metadata":{"id":"BNNigwADqIjd"}},{"cell_type":"code","source":["files.upload();"],"metadata":{"id":"IsvBcEl7qGa8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cm997whNLuP"},"source":["#2 Loading of all the generators"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfCB0Z44RDzz"},"outputs":[],"source":["from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J922PrOlTwls"},"outputs":[],"source":["class UNetDown(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4, normalization_type = None, dropout = 0.0, activation = None):\n","\n","    super(UNetDown, self).__init__()\n","\n","    #if batchnorm/instancenorm used, bias not used\n","\n","    use_bias = normalization_type == None\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","        \n","    if activation == None:\n","      layers.append(nn.LeakyReLU(negative_slope = 0.2))\n","\n","    if activation == \"ReLU\":\n","\n","      layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x):\n","\n","    return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"deGc1E9ST2_8"},"outputs":[],"source":["class UNetUp(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels, kernel_size = 4,  normalization_type = None, dropout = 0.0):\n","\n","    super(UNetUp, self).__init__()\n","\n","    use_bias = normalization_type == None\n","\n","    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, 2, 1, bias = use_bias)]\n","\n","    if not use_bias:\n","      if normalization_type == \"instance\":\n","\n","        layers.append(nn.InstanceNorm2d(out_channels))\n","\n","      else:\n","\n","        layers.append( nn.BatchNorm2d(out_channels))\n","\n","    layers.append(nn.ReLU())\n","\n","    if dropout:\n","\n","      layers.append(nn.Dropout(p = dropout))\n","\n","    self.model = nn.Sequential(*layers)\n","\n","\n","  def forward(self, x, skip = None):\n","      x = self.model(x)\n","      if skip is not None:\n","\n","        x = torch.cat((skip, x), 1)\n","\n","      return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B34EsSXxT_Nr"},"outputs":[],"source":["class GeneratorUNet(nn.Module):\n","\n","  def __init__(self, in_channels = 1, out_channels = 2, num_down = 8, ngf = 64, normalization_type = None):\n","\n","    super(GeneratorUNet, self).__init__()\n","\n","    self.downs = nn.ModuleList()\n","    self.ups = nn.ModuleList()\n","    \n","\n","    features =[ngf]\n","\n","    for i in range(3):\n","\n","      features.append(features[i]*2)\n","\n","    features.append(features[-1])\n","    #64, 128, 256, 512, 512\n","\n","    if num_down > 5:\n","\n","      features += [ngf * 8 for i in range(num_down - 5)]\n","    #for num_down = 8: 64, 128, 256, 512, 512, 512, 512, 512 (->1x1 for input size 256x256)\n","\n","\n","    #ENCODER (CONTRACTING PATH)\n","\n","    #outermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, ngf, 4))\n","\n","    in_channels = ngf #new in_channels for the next down-block\n","    \n","    for i,n_features in enumerate(features[1:len(features)-1]):\n","      #no dropout\n","      self.downs.append(UNetDown(in_channels, n_features, 4, normalization_type, 0.0))\n","      in_channels = n_features\n","\n","    \n","    #innermost down block: no normalization and no dropout, only downconv\n","    self.downs.append(UNetDown(in_channels, features[-1], 4, activation = \"ReLU\"))\n","    \n","\n","    #DECODER (EXPANSIVE PATH)\n","    i_channels = in_channels\n","    for i, n_features in enumerate((features[-2::-1])):\n","      \n","      \n","      #if i == 0, innermost(bottleneck), namely a block such that after down we go up. no dropout\n","      i_channels = in_channels if i == 0  else i_channels * 2\n","\n","      #no dropout for the first up and the last 4 ups \n","      dropout = 0.0 if (i == 0 or i  > 3) else 0.5\n","\n","      self.ups.append(UNetUp(i_channels, n_features, 4, normalization_type, dropout))\n","      i_channels = n_features\n","    \n","    \n","    self.final = nn.Sequential(\n","        nn.ConvTranspose2d(ngf*2,out_channels, kernel_size=4, stride=2, padding=1),\n","        nn.Tanh()\n","    )\n","\n","\n","\n","  def forward(self, x):\n","\n","    skip_connections = list()\n","\n","    #encoder\n","    for down in self.downs:\n","\n","      x = down(x)\n","      skip_connections.append(x)\n","\n","    #decoder with skip connections\n","    for i, up in enumerate(self.ups):\n","      \n","      x = up(x, skip_connections[-i-2])\n","\n","    return self.final(x)"]},{"cell_type":"markdown","source":["## 2.1 Connect to drive and load the .pt for the generators"],"metadata":{"id":"uqh3KsaCcnBc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i15rRWLzPPQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675951103498,"user_tz":-60,"elapsed":20831,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"5ec519d5-8fd4-4081-8f22-1dc234dd6609"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og6dkepnPVNf"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/TrainedNets/WGAN_9k_120.pt /content/ #WGAN generator"]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_big.pt /content/ # cGAN generator with 16k dataset"],"metadata":{"id":"sNWCGTAUlunn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_16_100.pt /content/ # cGAN generator with 9.6k dataset"],"metadata":{"id":"Sdw94A0qn6d7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_32_100.pt /content/"],"metadata":{"id":"xVTzi8c4q-Eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/cGAN_small_8.pt /content/"],"metadata":{"id":"RkVu1fdvMW1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIh__oBuNSJ7"},"outputs":[],"source":["# Add the .pt you want to test\n","G_paths = [\"WGAN_9k_120.pt\", \"cGAN_big.pt\", \"cGAN_small_16_100.pt\", \"cGAN_small_32_100.pt\", \"cGAN_small_8.pt\" ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Batve4YsQLBR"},"outputs":[],"source":["def load_generator(G, path = \"/content/cGAN-gen.pt\"):\n","  G.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ycoYUXeRyLR"},"outputs":[],"source":["Gs = [] # Array in which we store all the generators\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","for i, G_path in enumerate(G_paths):\n","\n","  Gs.append( GeneratorUNet(1,2,8,64, \"batchnorm\").to(device) )\n","  Gs[i].eval()\n","\n","  load_generator(Gs[i], \"/content/\" + G_path)"]},{"cell_type":"markdown","metadata":{"id":"JKYMOUa_G26b"},"source":["# 3 Visualize Results: show results with some test images"]},{"cell_type":"markdown","source":["With the following cells you can plot the results with some test images and save them in a jpeg image."],"metadata":{"id":"OckBxFv7pkTT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7noCSFOtHY-Q"},"outputs":[],"source":["test_animals_paths = read_lines(\"test_animals.txt\")\n","print(f\"{len(test_animals_paths)} animal images for testing\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nT_h3up8HvIM"},"outputs":[],"source":["test_coco_paths = []\n","\n","for path in paths:\n","  \n","  if path not in training_paths:\n","    test_coco_paths.append(path)\n","\n","print(f\"{len(test_coco_paths)} coco images for testing\")\n"]},{"cell_type":"markdown","metadata":{"id":"hRll8rzmI1j_"},"source":["## 3.1 Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfVGq77MHvPv"},"outputs":[],"source":["SIZE = 256\n","\n","test_transform = transforms.Compose([\n","                transforms.Resize((SIZE, SIZE),  transforms.InterpolationMode.BILINEAR),\n","                #transforms.RandomHorizontalFlip(),\n","            ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSRZsCzCJDkL"},"outputs":[],"source":["class GrayToColorDataset(Dataset):\n","\n","  def __init__(self, paths, transform = None):\n","    \n","    self.paths = paths\n","    self.transform = transform\n","\n","  def __len__(self):\n","\n","    return len(self.paths)\n","\n","  def __getitem__(self, idx):\n","\n","    img_rgb = Image.open(self.paths[idx]).convert(\"RGB\")\n","    img_rgb = self.transform(img_rgb)\n","    img_rgb = np.array(img_rgb)\n","\n","    #RGB -> Lab\n","    img_lab = rgb2lab(img_rgb).astype(\"float32\")\n","    img_lab = transforms.ToTensor()(img_lab)\n","\n","    #to have values in range [-1,1]\n","    L = img_lab[[0],:]/50. - 1.\n","    ab = img_lab[[1,2],:] / 110.\n","\n","    return (L,ab)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBPWKFJMJICR"},"outputs":[],"source":["test_coco_dataset = GrayToColorDataset(test_coco_paths, test_transform)\n","test_animals_dataset = GrayToColorDataset(test_animals_paths, test_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkgVd40XJZr4"},"outputs":[],"source":["\n","PIN_MEMORY = True\n","N_WORKERS = 2\n","BATCH_SIZE = 9\n","\n","test_coco_dataloader = DataLoader(test_coco_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n","                            pin_memory=PIN_MEMORY, shuffle = False)\n","\n","test_animals_dataloader = DataLoader(test_animals_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS,\n","                            pin_memory=PIN_MEMORY, shuffle = True)"]},{"cell_type":"markdown","metadata":{"id":"5LOVOSm0b9QU"},"source":["## 3.2 Plot and save results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1KbHvSvLIH5"},"outputs":[],"source":["def convert_lab_to_rgb(L, ab):\n","\n","  \"\"\"\n","  Provided a Lab image or a batch of Lab images, it returns it/them in RGB format \n","  input:\n","    - L: torch.tensor\n","    - ab: torch.tensor\n","  \n","  output:\n","    - img: numpy.ndarray (the rgb images)\n","  \"\"\"\n","\n","  #check shape (one image or a batch)\n","\n","  is_batch = len(ab.shape) > 3\n","  \n","  L = (L+1.)*50.\n","  ab = ab*110.\n","\n","  if is_batch:\n","    # input tensors: N x 1 x 256 x 256, N x 2 x 256 x 256\n","    Lab_images = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().detach().numpy()\n","  else:\n","    # input tensors: 1 x 256 x 256, 2 x 256 x 256\n","    Lab_image = torch.cat([L, ab], dim=0).permute(1, 2, 0).cpu().detach().numpy()\n","    return lab2rgb(Lab_image)\n","\n","  rgb_images = list()\n","\n","  for image in Lab_images:\n","\n","    img_rgb = lab2rgb(image)\n","    rgb_images.append(img_rgb)\n","\n","  return np.stack(rgb_images, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLlZtQmDLANa"},"outputs":[],"source":["def show_results(Ls, real_abs, fake_abs, path):\n","\n","  \"\"\"\n","  provided a batch of real and fake images, visualize them (+ the gray images)\n","  input:\n","    - Ls: batch with L for each image, N x 1 x 256 x 256 tensor\n","    - real_abs: batch with ab for each real image, N x 2 x 256 x 256 tensor\n","    - fake_abs: batch with ab for each fake image, N x 2 x 256 x 256 tensor\n","  \"\"\"\n","\n","  n_cols = Ls.shape[0]\n","\n","  real_images = convert_lab_to_rgb(Ls, real_abs)\n","  fake_images = convert_lab_to_rgb(Ls, fake_abs)\n","\n","  fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n","\n","  for idx in range(3):\n","\n","    axes[0,idx].axis(\"off\")\n","    axes[0,idx].imshow(real_images[ 3*idx], aspect = \"auto\")\n","\n","    axes[1,idx].axis(\"off\")\n","    axes[1,idx].imshow(real_images[ 3*idx + 1], aspect = \"auto\")\n","\n","    axes[2,idx].axis(\"off\")\n","    axes[2,idx].imshow(real_images[ 3*idx + 2], aspect = \"auto\")\n","  plt.subplots_adjust(wspace=0.05, hspace = .05)\n","  plt.savefig(path + \"_real.jpg\")\n","  plt.show()\n","  \n","\n","  fig_fake, axes_fake = plt.subplots(3, 3, figsize=(20, 20))\n","\n","  for idx in range(3):\n","\n","    axes_fake[0,idx].axis(\"off\")\n","    axes_fake[0,idx].imshow(fake_images[ 3*idx ], aspect = \"auto\")\n","\n","    axes_fake[1,idx].axis(\"off\")\n","    axes_fake[1,idx].imshow(fake_images[ 3*idx + 1], aspect = \"auto\")\n","\n","    axes_fake[2,idx].axis(\"off\")\n","    axes_fake[2,idx].imshow(fake_images[ 3*idx + 2], aspect = \"auto\")\n","  plt.subplots_adjust(wspace=0.05, hspace = .05)\n","  plt.savefig(path + \"_fake.jpg\")\n","  plt.show()\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_w-5Nc7mgY38"},"outputs":[],"source":["def show_images_for_model(G, dataloader, path):\n","\n","  Ls, abs = next(iter(dataloader))\n","  Ls = Ls.to(device)\n","  abs = abs.to(device)\n","  abs_fake = G(Ls)\n","  show_results(Ls, abs, abs_fake, path)\n","  \n","\n"]},{"cell_type":"markdown","source":["Choose the model to test: change idx to choose a different model. Have a look at Gs list to select the proper index."],"metadata":{"id":"7N_yD8H9cHxm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdJpSiHeXjfL"},"outputs":[],"source":["import random\n","np.random.seed(123)\n","random.seed(10)\n","\n","show_images_for_model(Gs[5], test_animals_dataloader, \"cGAN_small_mix\")"]},{"cell_type":"markdown","source":["Now you can download the .jpg images"],"metadata":{"id":"0VsWxgoZcBLs"}},{"cell_type":"markdown","metadata":{"id":"j96HijJqr2eE"},"source":["# 5 Evaluate Generator - first metric"]},{"cell_type":"markdown","source":["## 5.1 Load classifiers"],"metadata":{"id":"LVjt3rrieCmR"}},{"cell_type":"markdown","source":["Load on colab the two .pt files for the two classifiers (one for colored images and the other for gray images)"],"metadata":{"id":"2he1dsR_fSzo"}},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/TrainedNets/vgg16-color.pt /content/"],"metadata":{"id":"brfwjMuzfPal"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixCb6qElr-2g"},"outputs":[],"source":["from torchvision import models, transforms\n","\n","C_c = models.vgg16(pretrained=True); #Classifier for color images\n","\n","C_c.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","\n","C_c.load_state_dict(torch.load(\"/content/vgg16-color.pt\"))\n","\n","C_c = C_c.to(device);\n","\n","\n","C_c.eval();"]},{"cell_type":"markdown","source":["## 5.2 Dataset and Dataloader"],"metadata":{"id":"X5H-m5puhEXc"}},{"cell_type":"code","source":["import os\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd"],"metadata":{"id":"7OIhGBywnEAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_test_dataset(test_path = \"test_animals.txt\"):\n","\n","  #starting path for the kaggle dataset\n","  start_path = '/content/animal_data/Animal Image Dataset/'\n","\n","  images = []\n","  labels = []\n","\n","  with open(test_path) as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","    for path in val_paths:\n","      label = path.split('/')[4]\n","      images.append(path)\n","      labels.append(label)\n","      \n","  data = {'Images':images, 'Labels':labels} \n","  data = pd.DataFrame(data) \n","\n","  lb = LabelEncoder()\n","  data['encoded_labels'] = lb.fit_transform(data['Labels'])\n","\n","  return data\n","\n"],"metadata":{"id":"IITFKzhymPtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = build_test_dataset(test_path = \"test_animals.txt\")"],"metadata":{"id":"F5sj1uJuoxM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform used by the classifiers\n","trans_classifier = transforms.Compose([\n","                  transforms.Resize((224,224)),\n","                  transforms.ToTensor(),\n","                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","                  ])\n","\n","# Transform used to convert the result from the generator to the input format for the classifier\n","trans_gan_to_classifier = transforms.Compose([\n","                  transforms.Resize((224,224)),\n","                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","                  ])\n","\n","# Transform for the GAN\n","trans_gan = transforms.Compose([\n","                transforms.Resize((256, 256),  transforms.InterpolationMode.BILINEAR),\n","            ])"],"metadata":{"id":"UGPYLv4ChKzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following dataset will allow us to get all the input for the classifiers and for the generator (in particular the color image, the grayscale image and the label)"],"metadata":{"id":"4tibhIn4tKki"}},{"cell_type":"code","source":["class Animals_Dataset(Dataset):\n","    def __init__(self, img_data, tr1, tr2):\n","        self.tr1 = tr1\n","        self.tr2 = tr2\n","        self.img_data = img_data\n","        \n","    def __len__(self):\n","        return len(self.img_data)\n","    \n","    def __getitem__(self, index):\n","        img_name = self.img_data.loc[index, 'Images']\n","\n","        #format for classifiers\n","        image = Image.open(img_name)\n","        image = image.convert('RGB')\n","        gray = image.convert('L')\n","        gray_image = gray.convert('RGB')\n","\n","        #format for cGAN, WGAN\n","        img_rgb = self.tr2(image)\n","        img_np = np.array(img_rgb)\n","        img_lab = rgb2lab(img_np).astype(\"float32\")\n","        img_lab = transforms.ToTensor()(img_lab)\n","\n","        L = img_lab[[0],:] /50.-1.\n","        \n","        \n","        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n","        \n","        if self.tr1 is not None:\n","            image = self.tr1(image)\n","            gray_image = self.tr1(gray_image)\n","        \n","        return image, gray_image, L, label"],"metadata":{"id":"XIYf0FEHhOmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = Animals_Dataset(data, trans_classifier, trans_gan)"],"metadata":{"id":"RXCs9sMmhRWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n","                                                shuffle = False)"],"metadata":{"id":"GE5Tj-bbo45H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Function for the evaluation"],"metadata":{"id":"_HbLkXGDpxIH"}},{"cell_type":"code","source":["def evaluate_generator(G):\n","\n","  softmax = nn.Softmax(dim = 1)\n","  G.eval()\n","\n","  weighted_sum_fake = 0.0\n","  to_normalize = 0.0\n","\n","  weighted_sum_real = 0.0\n","  weighted_sum_gray = 0.0\n","\n","  sum_fake = 0.0\n","  sum_real = 0.0\n","  sum_gray = 0.0\n","\n","  accuracy_fake = 0.0\n","  accuracy_color = 0.0\n","  accuracy_gray = 0.0\n","\n","  for batch in tqdm(test_loader):\n","\n","    with torch.no_grad():\n","      image = batch[0].to(device)\n","      gray = batch[1].to(device)\n","      L = batch[2].to(device)\n","      labels = batch[3].to(device)\n","\n","      #FORWARD\n","\n","      prob_color_s = softmax(C_c(image))\n","      prob_gray_c = softmax(C_c(gray))\n","\n","      ab_fake = G(L)\n","\n","      #from GAN output to RGB (input classifier)\n","      rgb_fake = torch.from_numpy(convert_lab_to_rgb(L, ab_fake)).permute(0,3,1,2)\n","      rgb_fake = trans_gan_to_classifier(rgb_fake).to(device)\n","\n","      #output_fake: output C_c with fake images \n","      output_fake = C_c(rgb_fake) # logits\n","      \n","      #labels predicted by C_c with fake images\n","      pred_labels_fake = torch.argmax(output_fake, dim = 1)\n","\n","\n","      #select probs for the correct classes\n","      prob_color = prob_color_s[np.arange(len(prob_color_s)),labels]\n","      prob_fake = softmax(output_fake)[np.arange(len(prob_gray_c)),labels]\n","\n","      prob_gray = prob_gray_c[np.arange(len(prob_gray_c)),labels]\n","\n","      #weights computation\n","      weights = torch.abs(prob_color - prob_gray)\n","\n","      #update unnormalized sum and sum of weights\n","      weighted_sum_fake += torch.sum(weights*prob_fake)\n","      weighted_sum_real += torch.sum(weights*prob_color)\n","      weighted_sum_gray += torch.sum(weights*prob_gray)\n","\n","\n","      to_normalize += torch.sum(weights)\n","\n","      sum_fake += torch.sum(prob_fake)\n","      sum_gray += torch.sum(prob_gray)\n","      sum_real += torch.sum(prob_color)\n","\n","      #ACCURACY\n","      accuracy_fake += torch.sum(pred_labels_fake == labels)\n","      accuracy_color += torch.sum(torch.argmax(prob_color_s, dim= 1) == labels)\n","      accuracy_gray += torch.sum(torch.argmax(prob_gray_c, dim = 1) == labels)\n","\n","\n","  weighted_multinoulli_pred_real = (weighted_sum_real / to_normalize).item()\n","  weighted_multinoulli_pred_gray = (weighted_sum_gray / to_normalize).item()\n","  weighted_multinoulli_pred_fake = (weighted_sum_fake/ to_normalize).item()\n","\n","  multinoulli_pred_fake = (sum_fake / len(test_dataset) ).item()\n","  multinoulli_pred_real = (sum_real / len(test_dataset) ).item()\n","  multinoulli_pred_gray = (sum_gray / len(test_dataset) ).item()\n","\n","  accuracy_fake = ( accuracy_fake / len(test_dataset) ).item()\n","  accuracy_real = ( accuracy_color / len(test_dataset) ).item()\n","  accuracy_gray = ( accuracy_gray / len(test_dataset) ).item()\n","\n","  print(f\"Weighting metric, fake: {weighted_multinoulli_pred_fake}\")\n","  print(f\"Weighting metric, real: {weighted_multinoulli_pred_real}\") \n","  print(f\"Weighting metric, gray: {weighted_multinoulli_pred_gray}\")\n","  \n","  print()\n","\n","  print(f\"Accuracy metric, fake: {accuracy_fake}\")\n","  print(f\"Accuracy metric, real: {accuracy_real}\") \n","  print(f\"Accuracy metric, gray: {accuracy_gray}\") "],"metadata":{"id":"gdyctAb7p2aB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","#[\"WGAN_9k_120.pt\", \"cGAN_big.pt\", \"cGAN_small_16_100.pt\", \"cGAN_small_32_100.pt\", \"cGAN_small_8.pt\" ]"],"metadata":{"id":"k0zIuM42yedf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[0])"],"metadata":{"id":"XXBbjQulm3rU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[1])"],"metadata":{"id":"Jb06A55jlexy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[2])"],"metadata":{"id":"Mm_erF_bokpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[3])"],"metadata":{"id":"GTI1uIPXOt5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    evaluate_generator(Gs[4])"],"metadata":{"id":"qea7BoRdOuEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Weight comparison - second metric"],"metadata":{"id":"xu9gJzs-GHWL"}},{"cell_type":"markdown","source":["## 6.1 Dataset and dataloader"],"metadata":{"id":"2BKt1ziwNWNf"}},{"cell_type":"code","source":["def build_dataset(path_noadd = \"data_big_training.txt\"):\n","\n","  #starting path for the kaggle dataset\n","  start_path = '/content/animal_data/Animal Image Dataset/'\n","\n","  with open(path_noadd) as file:\n","    val_paths = [line.rstrip() for line in file]\n","\n","  images = []\n","  labels = []\n","\n","  for folders, subfolders, files in os.walk(start_path,topdown=True):\n","    label = folders.split('/')[4]\n","    for file in files:\n","\n","      path_file = start_path + label + '/' + file\n","\n","      if path_file not in val_paths:  \n","        images.append(path_file)\n","        labels.append(label)\n","      \n","  data = {'Images':images, 'Labels':labels} \n","  data = pd.DataFrame(data) \n","\n","  lb = LabelEncoder()\n","  data['encoded_labels'] = lb.fit_transform(data['Labels'])\n","\n","  return data"],"metadata":{"id":"bvnVfejhGA22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_data = build_dataset(path_noadd = \"data_big_training.txt\")\n","print(len(new_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2fYB-80GA23","executionInfo":{"status":"ok","timestamp":1675877204809,"user_tz":-60,"elapsed":2422,"user":{"displayName":"Simone Cecchinato","userId":"00241729136264729633"}},"outputId":"891bd883-cac3-4328-8a31-50da55de5af6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12978\n"]}]},{"cell_type":"code","source":["classifier_dataset = Animals_Dataset(new_data,trans_classifier, trans_gan)"],"metadata":{"id":"eQH8BCl0GA23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_loader = torch.utils.data.DataLoader(classifier_dataset, batch_size=128,\n","                                                shuffle = True)"],"metadata":{"id":"kodP5m7dGA23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.2 Classifier training"],"metadata":{"id":"2V2pxupeQC2E"}},{"cell_type":"markdown","source":["In this second metric we train two different networks. One with the original colored images and one with the images colored by the GAN. If the colorization is good then the weights of the two trained models shouldn't be very different."],"metadata":{"id":"y7B48FstQUqo"}},{"cell_type":"markdown","source":["## 6.2.1 Train first model"],"metadata":{"id":"oJJUDhU9Qs1f"}},{"cell_type":"markdown","source":["Train this model with the original colored images"],"metadata":{"id":"XwNY_5ETQ68m"}},{"cell_type":"code","source":["from torchvision import models, transforms\n","\n","classifier_model = models.vgg16(pretrained=True)\n","classifier_model.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","classifier_model = classifier_model.to(device)"],"metadata":{"id":"C_C9QsrWGA23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(classifier_model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(classifier_loader)"],"metadata":{"id":"RqEtR4-UGA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs = 2\n","print_every = 25\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc = []\n","train_loss = []\n","train_acc = []\n","total_step = len(classifier_loader)\n","\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","    # scheduler.step(epoch)\n","    correct = 0\n","    total=0\n","    print(f'Epoch {epoch}\\n')\n","\n","    for i, (images, _, _, labels) in tqdm(enumerate(classifier_loader), total = len(classifier_loader)):\n","\n","        # Move tensors to the configured device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = classifier_model(images)\n","        #print(outputs)\n","        #print(labels)\n","        loss = criterion(outputs, labels)\n","        #print(loss)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        _,pred = torch.max(outputs, dim=1)\n","        correct += torch.sum(pred==labels).item()\n","        total += labels.size(0)\n","\n","        if (i) % print_every == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, i, total_step, loss.item()))\n","            \n","    train_acc.append(100 * correct / total)\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","    classifier_model.train()"],"metadata":{"id":"tGEG8nFdGA24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save the trained network"],"metadata":{"id":"7nYozDs9Qxo2"}},{"cell_type":"code","source":["torch.save(classifier_model.state_dict(), 'vgg16_real_images.pt')"],"metadata":{"id":"OvJX7OhHGA24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.2.2 Train second model"],"metadata":{"id":"BayxxPzQGA24"}},{"cell_type":"markdown","source":["Train this model with the images colored by the GAN"],"metadata":{"id":"oRmROxE-Q-3N"}},{"cell_type":"code","source":["from torchvision import models, transforms\n","\n","classifier_fake_images = models.vgg16(pretrained=True)\n","classifier_fake_images.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","classifier_fake_images = classifier_fake_images.to(device)"],"metadata":{"id":"91Sb5qbvGA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.005\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer_fake = torch.optim.SGD(classifier_fake_images.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n","\n","# Train the model\n","total_step = len(classifier_loader)"],"metadata":{"id":"PLef5ApqGA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_classifier_generator(G, epochs = 2, print_every = 25):\n","\n","  n_epochs = 2\n","  print_every = 25\n","  valid_loss_min = np.Inf\n","  val_loss = []\n","  val_acc = []\n","  train_loss = []\n","  train_acc = []\n","  total_step = len(classifier_loader)\n","\n","  G.train()\n","  classifier_fake_images.train()\n","\n","  for epoch in range(1, n_epochs+1):\n","      running_loss = 0.0\n","      # scheduler.step(epoch)\n","      correct = 0\n","      total=0\n","      print(f'Epoch {epoch}\\n')\n","\n","      for i, (images, _, L, labels) in tqdm(enumerate(classifier_loader), total = len(classifier_loader)):\n","\n","          # Move tensors to the configured device\n","          images = images.to(device)\n","          labels = labels.to(device)\n","          L = L.to(device)\n","          ab = G(L).detach()\n","\n","          rgb_fake = torch.from_numpy(convert_lab_to_rgb(L, ab)).permute(0,3,1,2)\n","          rgb_fake = trans_gan_to_classifier(rgb_fake).to(device).detach()\n","\n","          optimizer_fake.zero_grad()\n","\n","          outputs = classifier_fake_images(rgb_fake)\n","          loss = criterion(outputs, labels)\n","\n","\n","          loss.backward()\n","          optimizer_fake.step()\n","          \n","          # print statistics\n","          running_loss += loss.item()\n","          _,pred = torch.max(outputs, dim=1)\n","          correct += torch.sum(pred==labels).item()\n","          total += labels.size(0)\n","\n","          if (i) % print_every == 0:\n","              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                    .format(epoch, n_epochs, i, total_step, loss.item()))\n","              \n","      train_acc.append(100 * correct / total)\n","      train_loss.append(running_loss/total_step)\n","      print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n","\n","      classifier_fake_images.train()\n"],"metadata":{"id":"iiMUYrr-v8HN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    train_classifier_generator(Gs[4])"],"metadata":{"id":"_X6L9MF_0oi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(classifier_fake_images.state_dict(), 'classifier_cGAN_small_8.pt')"],"metadata":{"id":"EGfRc3cq-5hl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To save the classifier"],"metadata":{"id":"GAEVK0uEG2Ln"}},{"cell_type":"code","source":["!cp /content/classifier_cGAN_small_8.pt /content/drive/MyDrive/TrainedNets/Weight_Comparison"],"metadata":{"id":"kvoFiufqG1LH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.3 Weights comparison"],"metadata":{"id":"goV7Fo1CNdCX"}},{"cell_type":"markdown","source":["Apply the L2-norm to the weights of the two models. If this value is small then the recolorization process is good."],"metadata":{"id":"56D9woGyRRJ1"}},{"cell_type":"code","source":["def weights_distance(m_1, m_2) :\n","\n","  m_1_list = list()\n","  for name, p in m_1.named_parameters():\n","    m_1_list.append(p)\n","\n","  m_2_list = list()\n","  for name, p in m_2.named_parameters():\n","    m_2_list.append(p)\n","\n","\n","  diff = list()\n","\n","  for i in range(len(m_1_list)):\n","    diff.append(torch.sum(torch.sub(m_1_list[i], m_2_list[i])**2))\n","\n","\n","  distance = 0.0\n","\n","  for i in range(len(diff)) :\n","    distance += diff[i]\n","\n","  return torch.sqrt(distance).item()"],"metadata":{"id":"b0gEkH45GA25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The smaller the distance, the better it is."],"metadata":{"id":"TdRpFSKINkVC"}},{"cell_type":"code","source":["distance = weights_distance(classifier_model, classifier_fake_images)\n","\n","print(distance) #l2-norm between the weights of the two models"],"metadata":{"id":"q6yDsqsrNiDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.4 Importing saved model to evaluate weight distance"],"metadata":{"id":"E0W19w5r6WPD"}},{"cell_type":"code","source":["!cp /content/drive/MyDrive/TrainedNets/Weight_Comparison/classifier_cGAN_big.pt /content/"],"metadata":{"id":"6wDh0QbuGvYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m_2 = models.vgg16(pretrained=True)\n","m_2.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","m_2.load_state_dict(torch.load(\"/content/vgg16_real_images.pt\"))\n","\n","m_2 = m_2.to(device)"],"metadata":{"id":"Jw2mGDts6U72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m_1 = models.vgg16(pretrained=True)\n","m_1.classifier[6] = nn.Linear(in_features=4096, out_features=12)\n","\n","m_1.load_state_dict(torch.load(\"/content/classifier_cGAN_small_32.pt\"))\n","\n","m_1 = m_1.to(device)"],"metadata":{"id":"VENZkGVNOqxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["distance = weights_distance(m_1, m_2)\n","\n","print(distance) #l2-norm between the weights of the two models"],"metadata":{"id":"QcYws38bEaQh"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rfMWhK6X_Pwos5oAaeKw9xH-b3FCDb14","timestamp":1675173022259}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}